{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [1] Import Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [1.1] Install Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1692678406969
        }
      },
      "outputs": [],
      "source": [
        "# %pip install azureml-dataset-runtime\n",
        "# %pip install -U azureml-fsspec\n",
        "# %pip install azureml.pipeline\n",
        "# %pip install azureml-explain-model\n",
        "# %pip install azureml-interpret\n",
        "# %pip install azureml.fsspec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [1.2] Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1691220498808
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4.46 ms, sys: 4.01 ms, total: 8.47 ms\n",
            "Wall time: 29.2 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Import required Azure Packages\n",
        "from azureml.core import Workspace, Dataset, Datastore, Environment, Experiment\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core.runconfig import RunConfiguration, DockerConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter, TrainingOutput\n",
        "from azureml.pipeline.steps import PythonScriptStep, AutoMLStep\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.pipeline.core.graph import PipelineParameter\n",
        "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [2] Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Print Current Date: 20_09_2023\n",
            "Experiment folder will be created with name: 20_09_2023_07_55_00\n"
          ]
        }
      ],
      "source": [
        "date_today = dt.date.today().strftime('%d_%m_%Y')\n",
        "datetime_now = dt.datetime.now().strftime('%d_%m_%Y_%H_%M_%S')\n",
        "print('Print Current Date:', date_today)\n",
        "print('Experiment folder will be created with name:', datetime_now)\n",
        "# print(f'For production we will run model on previous month i.e. month: {dt.date.today().month}, to predict results for next 2 months.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [2.1] Arguments for Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User name - specify your user name (this name will be used for creating base directories,  dataAsset naming)\n",
        "# Make sure to keep this unique & short\n",
        "user_name = 'master'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model name for deployment of model in azure. It will auto-increment the version everytime it runs.\n",
        "model_name = f'{user_name}_ChurnPrediction_Model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# new_data_flag seperates the model training pipeline (mtp) & new data pipeline (ndp).\n",
        "# new_data_flag is False: Run the pipeline for trainig the model on train dataset:  model training pipeline (mtp)\n",
        "# new_data_flag is True: Run the pipeline for new dataset: new data pipeline (ndp).\n",
        "new_data_flag = False \n",
        "\n",
        "# flag only when new_data_flag = True - for NDP\n",
        "# schedule_pipeline_flag = True, to trigger the pipeline when new data is added to data_path_to_monitor path\n",
        "schedule_pipeline_flag = False\n",
        "\n",
        "# flag only when new_data_flag = True - for NDP\n",
        "# If latest_data_ndp_flag = True => Run the NDP pipeline on the latest month data, this will ignore variables observation_month_number & observation_year.\n",
        "# If latest_data_ndp_flag = False => Run the NDP pipeline on specific month data by setting observation_month_number & observation_year.\n",
        "latest_data_ndp_flag = False\n",
        "\n",
        "# flag only when new_data_flag = True - for NDP\n",
        "# Specify model version which needs to be used for predicting the new data set. To use latest model trained, use model_version_ndp = -1\n",
        "model_version_ndp = -1\n",
        "# model_version_ndp = 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use monitoring_flag = True to monitor the model's performance by comparing actual vs predicted churn flag\n",
        "monitoring_flag = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1691220501490
        }
      },
      "outputs": [],
      "source": [
        "# Observation Month: Month for which we will run the model and it will do the prediction for next 2 months\n",
        "# Define the observation month for data preparation and modeling. Historical features will be created for this observation month.\n",
        "# Use month number for: January: 1, February: 2, March: 3, April: 4, May: 5, June: 6, \n",
        "# July: 7, August: 8, September: 9, October: 10, November: 11, December: 12\n",
        "observation_month_number = 10\n",
        "observation_year = 2022\n",
        "\n",
        "# To run model on previous month - for production\n",
        "# observation_month_number = dt.date.today().month \n",
        "# observation_year = dt.date.today().year\n",
        "\n",
        "# Define the number of months to be considered for historical feature creation. \n",
        "# Historical features will be created from last n months till observation month. \n",
        "# Ex: If observation month is July(7), then it will create last 6 months till Feb(2) (observation month is included).\n",
        "historical_months = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1691220500521
        }
      },
      "outputs": [],
      "source": [
        "# Note: For the first time, set reuse_sample_flag flag as False\n",
        "\n",
        "# Use sample_flag as true for testing to select some sample customer ids, if sample_flag is False then it will run on entire data\n",
        "num_sample_customer_ids = 10000\n",
        "sample_flag = False\n",
        "\n",
        "# For the first time, make reuse_sample_flag flag as False\n",
        "# Set reuse_samle_flag to True for re-using the last created sample (if it is false, it will overwrite the last sample created)\n",
        "reuse_sample_flag = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters for Train Test Split Step\n",
        "random_state = 42\n",
        "test_size = 0.1\n",
        "\n",
        "# Parameters for outlier treatment step\n",
        "outliers_method = 'iForest'\n",
        "outliers_threshold = 0.05\n",
        "\n",
        "# Parameters for multiCollinearity treatment step\n",
        "multiCollinearity_method = 'pearson'\n",
        "multiCollinearity_threshold = 0.95\n",
        "\n",
        "# Parameters for Feature selection step\n",
        "# feature selection method\n",
        "featureSelection_method = 'lightGBM'\n",
        "# Select features with score greater than or equal to featureSelection_threshold\n",
        "# featureSelection_threshold = 0 means select features based on featureSelection_percentage\n",
        "featureSelection_threshold = 0 #0.001\n",
        "# Select top x percetange of features x/100 = featureSelection_percentage\n",
        "# featureSelection_threshold will eliminate some features and then select \n",
        "# x (featureSelection_percentage) percentage of features from those selected features\n",
        "# featureSelection_percentage = 1 means select features based on featureSelection_threshold\n",
        "featureSelection_percentage = 1\n",
        "\n",
        "# Parameters for normalization step\n",
        "# normalization_method = 'standard'\n",
        "normalization_method = 'skip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AutoML Step Pipeline Parameters(HyperParameter Tunning parameters):\n",
        "# primary_metric: The metric that Automated Machine Learning will optimize for model selection. \n",
        "# Automated Machine Learning collects more metrics than it can optimize. \n",
        "# You can use get_primary_metrics to get a list of valid metrics for your given task. \n",
        "# For more information on how metrics are calculated, \n",
        "# see https://docs.microsoft.com/azure/machine-learning/how-to-configure-auto-train#primary-metric.\n",
        "# primay metric has to be one of the metric that azure offeres see above link for the list\n",
        "primary_metric = 'norm_macro_recall'\n",
        "\n",
        "# If user_selected_metric = '', best model is selected based on primary_metric and if user_selected_metric = 'f1_score_macro' \n",
        "# then azure will run the automlstep with primary_metric first and then we select the best model based on the user_selected_metric.\n",
        "# For list of user_selected_metric refer link \n",
        "# https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2\n",
        "# Average precision summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, \n",
        "# with the increase in recall from the previous threshold used as the weight.\n",
        "# user_selected_metric = 'f1_score_macro'\n",
        "# user_selected_metric = 'average_precision_score_macro'\n",
        "user_selected_metric = ''\n",
        "\n",
        "# iterations: The total number of different algorithm and parameter combinations to test during an automated ML experiment. \n",
        "# If not specified, the default is 1000 iterations.\n",
        "# More iterations => more different model algorithms and more different hyperparameters to be tried.\n",
        "model_iterations = 40\n",
        "\n",
        "# Maximum amount of time in hours that all iterations combined can take before the experiment terminates. \n",
        "# Can be a decimal value like 0.25 representing 15 minutes. If not specified, the default experiment timeout is 6 days. \n",
        "# To specify a timeout less than or equal to 1 hour, make sure your dataset's size is not greater than 10,000,000 (rows times column) \n",
        "# or an error results.\n",
        "experiment_timeout_hours = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline step will use previously stored run details if no change is made in the data if allow_reuse_step = True\n",
        "# You can specify allow_reuse=False as a parameter of the Step. When allow_reuse is set to False, \n",
        "# the step run won’t be reused, and a new run will always be generated for the step during pipeline execution. \n",
        "# Default behavior of Pipelines is to set allow_reuse=True for steps.\n",
        "allow_reuse_step = True\n",
        "\n",
        "# Pipeline step will use previously stored output if no change is made in the data if regenerate_outputs = True\n",
        "# If regenerate_outputs is set to True for the Experiment.Submit() call, a new submit will always force generation of all step outputs, \n",
        "# and disallow data reuse for any step of this run. Once this run is complete, however, subsequent runs may reuse the results of this run. \n",
        "# Default behavior of Pipelines is to set regenerate_outputs=False for experiment submit calls.\n",
        "regenerate_outputs = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1691220502335
        }
      },
      "outputs": [],
      "source": [
        "# # Python scripts folder (.py files folder location)\n",
        "# script_data_transformation_folder = '../DataPreparation/DataTransformation/'\n",
        "# script_data_preprocess_folder = '../DataPreparation/DataPreprocess/'\n",
        "# script_modeling_folder = '../Modeling/'\n",
        "\n",
        "# # Input data raw files (path inside the blob container)\n",
        "# # input_full_raw_data_path = 'InputData/FullRawFiles'\n",
        "# input_raw_data_path = 'InputData/InputRawFiles'\n",
        "\n",
        "# # Input mapping files (path inside the blob container)\n",
        "# input_mapping_data_path = 'InputData/MappingFiles'\n",
        "\n",
        "# # For model training pipeline (MTP)\n",
        "# if not new_data_flag:\n",
        "#     # Sample file path\n",
        "#     input_sample_data_path = f'Users/{user_name}/MTP/InputData/SampleFiles'\n",
        "\n",
        "#     # Intermediate output files (path inside the blob container)\n",
        "#     output_data_intermediate_path = f'Users/{user_name}/MTP/Outputs/OutputDataset/{datetime_now}'\n",
        "\n",
        "#     # Output path for model info - latest run\n",
        "#     output_data_other_file_path = f'Users/{user_name}/MTP/Outputs/OutputOtherFiles'\n",
        "\n",
        "# # For new data pipeline (NDP)\n",
        "# if new_data_flag:\n",
        "#     # Sample file path\n",
        "#     input_sample_data_path = f'Users/{user_name}/NDP/InputData/SampleFiles'\n",
        "\n",
        "#     # Intermediate output files (path inside the blob container)\n",
        "#     output_data_intermediate_path = f'Users/{user_name}/NDP/Outputs/OutputDataset/{datetime_now}'\n",
        "\n",
        "#     # Output path for model info - latest run\n",
        "#     output_data_other_file_path = f'Users/{user_name}/NDP/Outputs/OutputOtherFiles'\n",
        "\n",
        "# # Path to monitor for changes if schedule_pipeline_flag = True\n",
        "# data_path_to_monitor = f'{input_raw_data_path}/transaction_data/'\n",
        "\n",
        "# # Config files - for scheduler\n",
        "# # This file keep tracks of the pipeline parameters. Modify the parameters to alter pipeline output.\n",
        "# scheduler_config_file='../Configurations/schedule_ndp_config.yaml'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Python scripts folder (.py files folder location)\n",
        "script_data_transformation_folder = '../DataPreparation/DataTransformation/'\n",
        "script_data_preprocess_folder = '../DataPreparation/DataPreprocess/'\n",
        "script_modeling_folder = '../Modeling/'\n",
        "\n",
        "# Input data raw files (path inside the blob container)\n",
        "# input_full_raw_data_path = 'InputData/FullRawFiles'\n",
        "if new_data_flag:\n",
        "    input_raw_data_path = 'InputData/InputRawFiles_VM'\n",
        "else:\n",
        "    input_raw_data_path = 'InputData/InputRawFiles'\n",
        "    \n",
        "# Input mapping files (path inside the blob container)\n",
        "input_mapping_data_path = 'InputData/MappingFiles'\n",
        "\n",
        "# For model training pipeline (MTP)\n",
        "if not new_data_flag:\n",
        "    # Sample file path\n",
        "    input_sample_data_path = f'Users/{user_name}/MTP/InputData/SampleFiles'\n",
        "\n",
        "    # Intermediate output files (path inside the blob container)\n",
        "    output_data_intermediate_path = f'Users/{user_name}/MTP/Outputs/OutputDataset/{datetime_now}'\n",
        "\n",
        "    # Output path for model info - latest run\n",
        "    output_data_other_file_path = f'Users/{user_name}/MTP/Outputs/OutputOtherFiles'\n",
        "\n",
        "# For new data pipeline (NDP)\n",
        "if new_data_flag:\n",
        "    # Sample file path\n",
        "    input_sample_data_path = f'Users/{user_name}/NDP/InputData/SampleFiles'\n",
        "\n",
        "    # Intermediate output files (path inside the blob container)\n",
        "    output_data_intermediate_path = f'Users/{user_name}/NDP/Outputs/OutputDataset/{datetime_now}'\n",
        "\n",
        "    # Output path for model info - latest run\n",
        "    output_data_other_file_path = f'Users/{user_name}/NDP/Outputs/OutputOtherFiles'\n",
        "\n",
        "# Path to monitor for changes if schedule_pipeline_flag = True\n",
        "data_path_to_monitor = f'{input_raw_data_path}/transaction_data/'\n",
        "\n",
        "# Config files - for scheduler\n",
        "# This file keep tracks of the pipeline parameters. Modify the parameters to alter pipeline output.\n",
        "scheduler_config_file='../Configurations/schedule_ndp_config.yaml'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1691220502820
        }
      },
      "outputs": [],
      "source": [
        "# Choose a name for your CPU cluster, script will create compute cluster automatically if not exist\n",
        "if new_data_flag:\n",
        "    amlcompute_cluster_name = f\"{user_name}-cpu-cluster-ndp\"\n",
        "else:\n",
        "    amlcompute_cluster_name = f\"{user_name}-cpu-cluster-mtp\"\n",
        "\n",
        "# Select number of nodes for autoscaling of cluster\n",
        "num_min_nodes = 0\n",
        "num_max_nodes = 10\n",
        "\n",
        "# Parallel process to be run by parallel functions\n",
        "njobs = num_max_nodes*2-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment Name: master_model_training_pipeline\n"
          ]
        }
      ],
      "source": [
        "# Specify Environment & Experiment name - This will be visible in Azure UI\n",
        "# environment_name = f'{user_name}_env'\n",
        "\n",
        "# Experiment Name For Model Training Pipeline & New Data Pipeline\n",
        "if sample_flag and not new_data_flag:\n",
        "    experiment_name = f'{user_name}_model_training_pipeline_test'\n",
        "elif not new_data_flag:\n",
        "    experiment_name = f'{user_name}_model_training_pipeline'\n",
        "elif new_data_flag and sample_flag:\n",
        "    experiment_name = f'{user_name}_new_data_pipeline_test'\n",
        "elif new_data_flag and not sample_flag:\n",
        "    experiment_name = f'{user_name}_new_data_pipeline'\n",
        "else:\n",
        "    experiment_name = f'{user_name}_test_pipeline'\n",
        "    \n",
        "print('Experiment Name:', experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provide suffix for naming the asset. Ex: transaction_data is the data asset name which script will create. \n",
        "# If dataAssetName_suffix = '', then default names will be used, specified in various sections of script.\n",
        "# If you specify dataAssetName_suffix = '_v1' then transaction_data asset name can be seen as -> username_transaction_data_v1\n",
        "# Suffix mtp: model training pipeline Ex: username_transaction_data_mtp\n",
        "# Suffix ndp: new data pipeline Ex: username_transaction_data_mtp\n",
        "dataAssetName_suffix = '_mtp'\n",
        "if new_data_flag:\n",
        "    dataAssetName_suffix = '_ndp'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "if new_data_flag == False:\n",
        "    # These flags can only be true for NDP\n",
        "    schedule_pipeline_flag = False\n",
        "    latest_data_ndp_flag = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [2.2] Azure ML Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1691220503536
        }
      },
      "outputs": [],
      "source": [
        "# Loads workspace info from config.json file\n",
        "ws = Workspace.from_config(path='../Configurations/', _file_name='workspace_config.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [2.3] Azure Blob Storage Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1691220504366
        }
      },
      "outputs": [],
      "source": [
        "# blob storage configurations\n",
        "blob_storage_name = 'azureconstorage3630ce4c4'\n",
        "blob_container = 'azureml'\n",
        "blob_key = 'OaxkL/pE4/XVJ1nJiZABE8wvsfSN434A66MiyiyywQOhYEDRYXS0t9DP+xLDT+RS8Alcygkszd+W+ASt54cY2w=='\n",
        "\n",
        "# Datastore name: We can see dataAssets created in this, this will also refer to the default blob storage specified\n",
        "# datastore_name = f\"{user_name}_blob_datastore\"\n",
        "datastore_name = \"bidv_blob_datastore\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [2.4] Other Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1691220499531
        }
      },
      "outputs": [],
      "source": [
        "# pd.set_option(\"display.max_columns\", None)\n",
        "# pd.set_option(\"display.max_rows\", 50)\n",
        "# pd.options.display.float_format = '{:.3f}'.format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [3] Reference Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [3.1] Create and Register DataStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1691220508545
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 80.1 ms, sys: 14.5 ms, total: 94.6 ms\n",
            "Wall time: 811 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'bidv_blob_datastore'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# Blob storage to Datastore\n",
        "blob_data_store = Datastore.register_azure_blob_container(\n",
        "    workspace = ws, \n",
        "    datastore_name = datastore_name,\n",
        "    account_name = blob_storage_name,\n",
        "    container_name = blob_container,\n",
        "    account_key = blob_key\n",
        ")\n",
        "\n",
        "# Default Datastore\n",
        "# default_store = ws.get_default_datastore()\n",
        "default_store = blob_data_store\n",
        "ws.set_default_datastore(name = datastore_name)\n",
        "\n",
        "default_store = ws.get_default_datastore()\n",
        "eval(str(default_store))['name']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [3.2] Create and Register Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [3.2.1] Input Mapping Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1691220514592
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Referencing master_mapping_data_occupation_mtp data aset on azure. Use mapping_data_occupation in notebook to reference it.\n",
            "Referencing master_mapping_data_translation_mtp data aset on azure. Use mapping_data_translation in notebook to reference it.\n",
            "Referencing master_mapping_data_region_mtp data aset on azure. Use mapping_data_region in notebook to reference it.\n",
            "CPU times: user 262 ms, sys: 18.1 ms, total: 280 ms\n",
            "Wall time: 2.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# List of mapping csv files to read  - use only csv files\n",
        "map_dataAsset_mappingFile = {\n",
        "    #'mapping_data_mstatus' : 'mapping_mstatus.csv',\n",
        "    'mapping_data_occupation' : 'mapping_occupation.csv',\n",
        "    'mapping_data_translation' : 'mapping_vn_en.csv',\n",
        "    'mapping_data_region' : 'mapping_region.csv'\n",
        "}\n",
        "\n",
        "# Create and Register Data Assets\n",
        "for dataAssetName, mappingFileName in map_dataAsset_mappingFile.items():\n",
        "    dataAssetNameOnAzure = f'{user_name}_{dataAssetName}{dataAssetName_suffix}'\n",
        "    csv_path = [(blob_data_store, f\"{input_mapping_data_path}/{mappingFileName}\")]\n",
        "    try:\n",
        "        globals()[dataAssetName] = Dataset.get_by_name(ws, dataAssetNameOnAzure)\n",
        "        print(f'Referencing {dataAssetNameOnAzure} data aset on azure. Use {dataAssetName} in notebook to reference it.')\n",
        "    except:\n",
        "        globals()[dataAssetName] = Dataset.Tabular.from_delimited_files(path=csv_path)\n",
        "        globals()[dataAssetName] = eval(dataAssetName).register(\n",
        "            ws, dataAssetNameOnAzure, create_new_version=True, \n",
        "            tags={'step':'input', 'data': 'mapping', 'sample_data':'false' ,'temporary_data': 'False'}\n",
        "        )\n",
        "        print(f'Created {dataAssetNameOnAzure} data aset on azure. Use {dataAssetName} here to reference it.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [4] Setup Environment & Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [4.1] Create & Register Custom Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create new enviroment\n",
        "# myenv = Environment(environment_name)\n",
        "\n",
        "# # Use conda_dependencies.yml to create a conda environment in the docker image\n",
        "# myenv.python.user_managed_dependencies = False\n",
        "\n",
        "# # Specify CondaDependencies\n",
        "# myenv_dependencies = CondaDependencies.create(\n",
        "#     conda_packages=['pandas', 'scikit-learn'],\n",
        "#     pip_packages= ['azureml-sdk[automl]', 'pyarrow', 'ipywidgets']\n",
        "# )\n",
        "# myenv.python.conda_dependencies = myenv_dependencies\n",
        "\n",
        "# # Register enviroment to workspace\n",
        "# myenv.register(ws)\n",
        "\n",
        "# print('Environment Created & Registerd to workspace.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [4.2] Create new or use an existing compute cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1691220522380
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing cluster master-cpu-cluster-mtp, will use that for pipeline.\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    aml_compute = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
        "    print(f'Found existing cluster {amlcompute_cluster_name}, will use that for pipeline.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2', max_nodes=1)\n",
        "    aml_compute = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
        "    print('Compute cluster created with name:', amlcompute_cluster_name)\n",
        "    \n",
        "# Check the cluster status\n",
        "aml_compute.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [4.3] Define Run Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1691220525715
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run configuration created.\n"
          ]
        }
      ],
      "source": [
        "# Create a new runconfig object\n",
        "aml_run_config = RunConfiguration()\n",
        "\n",
        "# Use the aml_compute cluster\n",
        "aml_run_config.target = aml_compute\n",
        "\n",
        "# Enable autoscaling\n",
        "aml_compute.update(min_nodes=num_min_nodes, max_nodes=num_max_nodes, idle_seconds_before_scaledown=1800)\n",
        "\n",
        "# Use myenv environment\n",
        "# aml_compute.environment = myenv\n",
        "\n",
        "# Enable Docker\n",
        "docker_config = DockerConfiguration(use_docker=True)\n",
        "aml_run_config.docker = docker_config\n",
        "\n",
        "# Use conda_dependencies.yml to create a conda environment in the docker image\n",
        "aml_run_config.environment.python.user_managed_dependencies = False\n",
        "\n",
        "# Specify CondaDependencies\n",
        "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
        "    conda_packages=['pandas', 'scikit-learn'],\n",
        "    pip_packages= [\n",
        "        'azureml-sdk[automl]==1.52.0', 'azureml-fsspec', 'pyarrow>=0.16.0', 'ipywidgets', 'xgboost', 'joblib'\n",
        "    ]\n",
        ")\n",
        "\n",
        "print('Run configuration created.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [5] Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Default Pipeline Input Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dynamic parameters to the pipeline - will be used while scheduling the pipeline\n",
        "# Specifying default values of these pipeline parameters\n",
        "# Change them in schedule_ndp.yaml file to use dynamic values during runtime\n",
        "pipeline_parameters_mtp = {\n",
        "    \"observation_year\": observation_year,\n",
        "    \"observation_month_number\" : observation_month_number,\n",
        "    \"historical_months\" : historical_months,\n",
        "    \"latest_data_ndp_flag\" : False, # this should always be false as this is for NDP\n",
        "}\n",
        "\n",
        "pipeline_parameters_ndp = {\n",
        "    \"model_version_ndp\" : model_version_ndp,\n",
        "    \"observation_year\": observation_year,\n",
        "    \"observation_month_number\" : observation_month_number,\n",
        "    \"historical_months\" : historical_months,\n",
        "    \"latest_data_ndp_flag\" : latest_data_ndp_flag\n",
        "}\n",
        "\n",
        "if new_data_flag:\n",
        "    pipeline_parameters = pipeline_parameters_ndp\n",
        "else:\n",
        "    pipeline_parameters = pipeline_parameters_mtp\n",
        "\n",
        "for key, value in pipeline_parameters.items():\n",
        "    globals()[f'pp_{key}'] = PipelineParameter(name=key, default_value=value)\n",
        "\n",
        "pp_model_name = PipelineParameter(\"model_name\", default_value=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining pipeline name & tags\n",
        "if new_data_flag:\n",
        "    # For NDP - new data pipeline\n",
        "    pipeline_name = f'{user_name}_Pipeline-DataPrep-NewData_{datetime_now}'\n",
        "    pipeline_tags = {\n",
        "        'pipeline type' : 'New Data Pipeline (NDP)', 'model_version' : pp_model_version_ndp,\n",
        "        'pipeline_name' : pipeline_name, \n",
        "        'username' : user_name, 'output_data_intermediate_path' : output_data_intermediate_path, \n",
        "        'new_data_flag' : new_data_flag, 'schedule_pipeline_flag' : schedule_pipeline_flag, \n",
        "        'latest_data_ndp_flag' : latest_data_ndp_flag, 'data_path_to_monitor' : data_path_to_monitor,\n",
        "        'sample_flag' : sample_flag, 'sample_size' : num_sample_customer_ids, 'reuse_sample_flag' : reuse_sample_flag,\n",
        "        'observation_month_number' : pp_observation_month_number, 'observation_year' : pp_observation_year, \n",
        "        'historical_months' : pp_historical_months, \n",
        "        'allow_reuse_step' : allow_reuse_step, 'regenerate_outputs' : regenerate_outputs,\n",
        "        'amlcompute_cluster_name' : amlcompute_cluster_name, 'num_min_nodes' : num_min_nodes, 'num_max_nodes' : num_max_nodes,\n",
        "        'test_size' : test_size, 'outliers_method' : outliers_method, 'outliers_threshold' : outliers_threshold,\n",
        "        'multiCollinearity_method' : multiCollinearity_method, 'multiCollinearity_threshold' : multiCollinearity_threshold,\n",
        "        'featureSelection_method' : featureSelection_method, 'featureSelection_threshold' : featureSelection_threshold, \n",
        "        'featureSelection_percentage' : featureSelection_percentage, 'normalization_method' : normalization_method,\n",
        "        'model_name' : model_name, 'primary_metric' : primary_metric, 'user_selected_metric' : user_selected_metric, \n",
        "    }\n",
        "else:\n",
        "    # For MTP - model training pipeline\n",
        "    pipeline_name = f'{user_name}_Pipeline-DataPrep-ModelTrainTest_{datetime_now}'\n",
        "    pipeline_tags = {\n",
        "        'pipeline type' : 'Model Training Pipeline (MTP)', 'model_version' : model_version_ndp,\n",
        "        'pipeline_name' : pipeline_name, \n",
        "        'username' : user_name, 'output_data_intermediate_path' : output_data_intermediate_path, \n",
        "        'new_data_flag' : new_data_flag, 'schedule_pipeline_flag' : schedule_pipeline_flag, \n",
        "        'latest_data_ndp_flag' : latest_data_ndp_flag, 'data_path_to_monitor' : data_path_to_monitor,\n",
        "        'sample_flag' : sample_flag, 'sample_size' : num_sample_customer_ids, 'reuse_sample_flag' : reuse_sample_flag,\n",
        "        'observation_month_number' : pp_observation_month_number, 'observation_year' : pp_observation_year, \n",
        "        'historical_months' : pp_historical_months,\n",
        "        'allow_reuse_step' : allow_reuse_step, 'regenerate_outputs' : regenerate_outputs,\n",
        "        'amlcompute_cluster_name' : amlcompute_cluster_name, 'num_min_nodes' : num_min_nodes, 'num_max_nodes' : num_max_nodes,\n",
        "        'test_size' : test_size, 'outliers_method' : outliers_method, 'outliers_threshold' : outliers_threshold,\n",
        "        'multiCollinearity_method' : multiCollinearity_method, 'multiCollinearity_threshold' : multiCollinearity_threshold,\n",
        "        'featureSelection_method' : featureSelection_method, 'featureSelection_threshold' : featureSelection_threshold, \n",
        "        'featureSelection_percentage' : featureSelection_percentage, 'normalization_method' : normalization_method,\n",
        "        'model_name' : model_name, 'primary_metric' : primary_metric, 'user_selected_metric' : user_selected_metric, \n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [5.1] Data Transformation Pipeline Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.1.1] Fetch Latest Data Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetch_Data step created.\n"
          ]
        }
      ],
      "source": [
        "# Step Overview:\n",
        "# Step will fetch data from input_raw_data_path path. Based on the months specified in \n",
        "# observation_year, observation_month_number & historical_months variables.\n",
        "# This step is common for both model training pipeline (mtp) & new data pipeline (ndp)\n",
        "\n",
        "# Define step variables\n",
        "step_output_data_name_demo = 'demographic_data'\n",
        "step_output_data_name_trx = 'transaction_data'\n",
        "step_output_data_name_acc = 'account_data'\n",
        "step_display_title = \"Fetch_Latest_Data\"\n",
        "step_script_name = 'fetch_latest_data.py'\n",
        "ws_details = ws.get_details()['id']\n",
        "\n",
        "# Creating output folder with name step_output_data_name & Registering it to DataAssets\n",
        "globals()[step_output_data_name_demo] = OutputFileDatasetConfig(\n",
        "    # name of the file in the blob\n",
        "    name = step_output_data_name_demo, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_demo}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_demo}{dataAssetName_suffix}',\n",
        "    # tags={'step':'output', 'data': 'raw', 'multiple_files' : 'True', 'sample_data':'False' ,'temporary_data': 'False'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "# Creating output folder with name step_output_data_name & Registering it to DataAssets\n",
        "globals()[step_output_data_name_trx] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_trx, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_trx}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_trx}{dataAssetName_suffix}',\n",
        "    description = f'{step_output_data_name_trx} for {pp_observation_year}_{pp_observation_month_number}',\n",
        "    # tags={\n",
        "    #     'step':'output', 'data': 'raw', \n",
        "    #     'observation_year' : pp_observation_year, 'observation_month' : pp_observation_month_number,\n",
        "    #     'historical_months' :  pp_historical_months,\n",
        "    #     'multiple_files' : 'True', 'sample_data':'False', 'temporary_data': 'True'\n",
        "    # }\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "# Creating output folder with name step_output_data_name & Registering it to DataAssets\n",
        "globals()[step_output_data_name_acc] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_acc, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_acc}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_acc}{dataAssetName_suffix}',\n",
        "    # tags={'step':'output', 'data': 'raw', 'multiple_files' : 'True', 'sample_data':'False' ,'temporary_data': 'False'}\n",
        "    tags = pipeline_tags\n",
        ")   \n",
        "\n",
        "# Creating the step\n",
        "fetchLatestDataStep = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name = step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path_demo\", eval(step_output_data_name_demo),\n",
        "        \"--output_data_path_trx\", eval(step_output_data_name_trx),\n",
        "        \"--output_data_path_acc\", eval(step_output_data_name_acc),\n",
        "        \"--observation_year\", pp_observation_year,\n",
        "        \"--observation_month_number\", pp_observation_month_number,\n",
        "        \"--historical_months\", pp_historical_months,\n",
        "        \"--input_raw_data_path\", input_raw_data_path,\n",
        "        \"--new_data_flag\", new_data_flag,\n",
        "        \"--monitoring_flag\", monitoring_flag,\n",
        "        \"--latest_data_ndp_flag\", pp_latest_data_ndp_flag,\n",
        "        \"--ws_details\", ws_details\n",
        "    ],\n",
        "    #inputs=[],\n",
        "    outputs=[\n",
        "        eval(step_output_data_name_trx), \n",
        "        eval(step_output_data_name_demo),\n",
        "        eval(step_output_data_name_acc)\n",
        "    ],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_transformation_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")\n",
        "\n",
        "print('Fetch_Data step created.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.1.2] Data Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling_Data step created.\n"
          ]
        }
      ],
      "source": [
        "# Step Overview:\n",
        "# Step will create the sample if reuse_sample_flag = False, it will return previous sample if reuse_sample_flag is True\n",
        "# This step is common for both model training pipeline (mtp) & new data pipeline (ndp)\n",
        "\n",
        "# Define step variables\n",
        "step_output_data_name_demo = 'sample_demographic_data'\n",
        "step_output_data_name_trx = 'sample_transaction_data'\n",
        "step_output_data_name_acc = 'sample_account_data'\n",
        "step_display_title = \"Sampling_Data\"\n",
        "step_script_name = 'data_sampling.py'\n",
        "\n",
        "# Creating output folder with name step_output_data_name & Registering it to DataAssets\n",
        "globals()[step_output_data_name_demo] = OutputFileDatasetConfig(\n",
        "    # name of the file in the blob\n",
        "    name = step_output_data_name_demo, \n",
        "    destination = (default_store, f'{input_sample_data_path}/{step_output_data_name_demo}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_demo}{dataAssetName_suffix}',\n",
        "    # tags={'step':'output', 'data': 'sampled', 'sample_data':'True' ,'temporary_data': 'False'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "# Creating output folder with name step_output_data_name & Registering it to DataAssets\n",
        "globals()[step_output_data_name_trx] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_trx, \n",
        "    destination = (default_store, f'{input_sample_data_path}/{step_output_data_name_trx}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_trx}{dataAssetName_suffix}',\n",
        "    # tags={'step':'output', 'data': 'sampled', 'sample_data':'True' ,'temporary_data': 'False'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "# Creating output folder with name step_output_data_name & Registering it to DataAssets\n",
        "globals()[step_output_data_name_acc] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_acc, \n",
        "    destination = (default_store, f'{input_sample_data_path}/{step_output_data_name_acc}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_acc}{dataAssetName_suffix}',\n",
        "    # tags={'step':'output', 'data': 'sampled', 'sample_data':'True' ,'temporary_data': 'False'}\n",
        "    tags = pipeline_tags\n",
        ")   \n",
        "\n",
        "# Creating the step\n",
        "samplingDataStep = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name = step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path_demo\", eval(step_output_data_name_demo),\n",
        "        \"--output_data_path_trx\", eval(step_output_data_name_trx),\n",
        "        \"--output_data_path_acc\", eval(step_output_data_name_acc),\n",
        "        \"--num_sample_customer_ids\", num_sample_customer_ids,\n",
        "        \"--reuse_sample_flag\", reuse_sample_flag\n",
        "    ],\n",
        "    inputs=[\n",
        "        transaction_data.read_parquet_files().as_input('raw_data_transaction'),\n",
        "        demographic_data.read_parquet_files().as_input('raw_data_demographics'),\n",
        "        account_data.read_parquet_files().as_input('raw_data_account'),\n",
        "    ],\n",
        "    outputs=[\n",
        "        eval(step_output_data_name_trx), \n",
        "        eval(step_output_data_name_demo), \n",
        "        eval(step_output_data_name_acc)\n",
        "    ],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_transformation_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")\n",
        "\n",
        "print('Sampling_Data step created.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.1.3] Prepare Demographics Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1691220526108
        }
      },
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will transform the demographics data.\n",
        "# This step is common for both model training pipeline (mtp) & new data pipeline (ndp)\n",
        "\n",
        "# Define step variables\n",
        "step_output_data_name = 'transformed_demographics_data'\n",
        "step_display_title = \"Transform_Demographics_Data\"\n",
        "step_script_name = 'transform_demographics.py'\n",
        "\n",
        "# Creating output folder with name step_output_data_name & Registering it to DataAssets\n",
        "globals()[step_output_data_name] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name}{dataAssetName_suffix}',\n",
        "    # tags={'step':'output', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "# Inputs for the step\n",
        "step_inputs = [\n",
        "    mapping_data_region.as_named_input('mapping_data_region'), \n",
        "    mapping_data_occupation.as_named_input('mapping_data_occupation'),\n",
        "    mapping_data_translation.as_named_input('mapping_data_translation')\n",
        "]\n",
        "# Selecting input data as raw or sample\n",
        "if sample_flag:\n",
        "    # If we are using the output from previous step, then we use .read_parquet_files().as_input() functions to send input to this step\n",
        "    raw_data_input=[sample_demographic_data.read_parquet_files().as_input('raw_data_demographics')] \n",
        "else:\n",
        "    # If both sample and reuse sample flags are false, then we run the code on entire data, readind as data assets\n",
        "    # raw_data_input=[demographics_data.as_named_input('raw_data_demographics')] # <- if reading direct previous data assets\n",
        "    raw_data_input=[demographic_data.read_parquet_files().as_input('raw_data_demographics')]\n",
        "# Combining mapping files with the raw or sample input data\n",
        "step_inputs = raw_data_input + step_inputs\n",
        "    \n",
        "# Creating the step\n",
        "transformingDemographicsStep = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name = step_script_name,\n",
        "    arguments=[\n",
        "        f\"--output_data_path\", eval(step_output_data_name)\n",
        "    ],\n",
        "    inputs=step_inputs,\n",
        "    outputs=[eval(step_output_data_name)],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_transformation_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.1.4] Prepare Transaction Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1691220526516
        }
      },
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will transform the transaction data.\n",
        "# This step is common for both model training pipeline (mtp) & new data pipeline (ndp)\n",
        "\n",
        "step_output_data_name = 'transformed_transaction_data'\n",
        "step_display_title = \"Transform_Transaction_Data\"\n",
        "step_script_name = 'transform_transaction.py'\n",
        "\n",
        "globals()[step_output_data_name] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name}{dataAssetName_suffix}',\n",
        "    # tags={'step':'output', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "# Inputs for the step\n",
        "step_inputs = [\n",
        "    mapping_data_translation.as_named_input('mapping_data_translation'),\n",
        "    transformed_demographics_data.read_parquet_files().as_input('transformed_demographics_data')\n",
        "]\n",
        "if sample_flag:\n",
        "    raw_data_input=[sample_transaction_data.read_parquet_files().as_input('raw_data_transaction')] \n",
        "else:\n",
        "    raw_data_input=[transaction_data.read_parquet_files().as_input('raw_data_transaction')]\n",
        "step_inputs = raw_data_input + step_inputs\n",
        "\n",
        "transformingTransactionStep = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path\", eval(step_output_data_name),\n",
        "        \"--njobs\", njobs,\n",
        "    ],\n",
        "    inputs=step_inputs,\n",
        "    outputs=[eval(step_output_data_name)],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_transformation_folder,\n",
        "    allow_reuse = allow_reuse_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.1.5] Prepare Account Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will transform the account data.\n",
        "# This step is common for both model training pipeline (mtp) & new data pipeline (ndp)\n",
        "\n",
        "step_output_data_name = 'transformed_account_data'\n",
        "step_display_title = \"Transform_Account_Data\"\n",
        "step_script_name = 'transform_account.py'\n",
        "\n",
        "globals()[step_output_data_name] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "# Inputs for the step\n",
        "step_inputs = [\n",
        "    mapping_data_translation.as_named_input('mapping_data_translation'),\n",
        "]\n",
        "if sample_flag:\n",
        "    raw_data_input=[sample_account_data.read_parquet_files().as_input('raw_data_account')] \n",
        "else:\n",
        "    raw_data_input=[account_data.read_parquet_files().as_input('raw_data_account')]\n",
        "step_inputs = raw_data_input + step_inputs\n",
        "\n",
        "transformingAccountStep = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path\", eval(step_output_data_name)\n",
        "    ],\n",
        "    inputs=step_inputs,\n",
        "    outputs=[eval(step_output_data_name)],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_transformation_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.1.6] Feature Engineering Transaction Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will create new features using the transaction data.\n",
        "# This step is common for both model training pipeline (mtp) & new data pipeline (ndp)\n",
        "\n",
        "step_output_data_name = 'feature_eng_transaction_data'\n",
        "step_display_title = \"Feature_Engineering-Transaction_Data\"\n",
        "step_script_name = 'feature_eng_transaction.py'\n",
        "\n",
        "globals()[step_output_data_name] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "featureEngTransactionStep = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path\", eval(step_output_data_name),\n",
        "        \"--observation_month_number\", pp_observation_month_number, \n",
        "        \"--observation_year\", pp_observation_year, \n",
        "        \"--historical_months\", pp_historical_months\n",
        "    ],\n",
        "    inputs=[transformed_transaction_data.read_parquet_files().as_input('transformed_transaction_data')],\n",
        "    outputs=[eval(step_output_data_name)],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_preprocess_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.1.7] Merging Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will merge transaction, demographic, account & feature engineered transaction data.\n",
        "# This step is common for both model training pipeline (mtp) & new data pipeline (ndp).\n",
        "\n",
        "step_output_data_name = 'merged_transformed_data'\n",
        "step_display_title = \"Merge_Transformed_Data\"\n",
        "step_script_name = 'merge_transformed_data.py'\n",
        "\n",
        "globals()[step_output_data_name] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "mergingTransformedDataStep = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path\", eval(step_output_data_name),\n",
        "        \"--observation_month_number\", pp_observation_month_number, \n",
        "        \"--observation_year\", pp_observation_year,\n",
        "    ],\n",
        "    inputs=[\n",
        "        feature_eng_transaction_data.read_parquet_files().as_input('feature_eng_transaction_data'),\n",
        "        transformed_transaction_data.read_parquet_files().as_input('transformed_transaction_data'),\n",
        "        transformed_demographics_data.read_parquet_files().as_input('transformed_demographics_data'),\n",
        "        transformed_account_data.read_parquet_files().as_input('transformed_account_data'), \n",
        "    ],\n",
        "    outputs=[eval(step_output_data_name)],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_transformation_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.1.8] Create Target Variable - Churn Flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will create churn flag for training and testing set.\n",
        "# This step is only for model training pipeline (mtp).\n",
        "\n",
        "step_output_data_name = 'created_churn_flag_data'\n",
        "step_display_title = \"Create_Target-Churn_Flag\"\n",
        "step_script_name = 'create_churn_flag.py'\n",
        "\n",
        "globals()[step_output_data_name] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "creatingChurnFlagStep = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path\", eval(step_output_data_name),\n",
        "        \"--njobs\", njobs,\n",
        "    ],\n",
        "    inputs=[\n",
        "        transformed_transaction_data.read_parquet_files().as_input('transformed_transaction_data'),\n",
        "    ],\n",
        "    outputs=[eval(step_output_data_name)],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_transformation_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.1.9] Merging Data with Target Churn Flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will merge churn flag with the previous merged data.\n",
        "# This step is only for model training pipeline (mtp).\n",
        "\n",
        "step_output_data_name = 'merged_with_churn_flag_data'\n",
        "step_display_title = \"Merge_Data_with_Churn_Flag\"\n",
        "step_script_name = 'merge_churn_flag.py'\n",
        "\n",
        "globals()[step_output_data_name] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "mergedWithChurnStep = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path\", eval(step_output_data_name),\n",
        "        \"--observation_month_number\", pp_observation_month_number, \n",
        "        \"--observation_year\", pp_observation_year,\n",
        "    ],\n",
        "    inputs=[\n",
        "        created_churn_flag_data.read_parquet_files().as_input('created_churn_flag_data'),\n",
        "        merged_transformed_data.read_parquet_files().as_input('merged_transformed_data'),\n",
        "    ],\n",
        "    outputs=[eval(step_output_data_name)],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_transformation_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [5.2] Data Preprocess Pipeline Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.2.1] Data Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will create training and testing set based on test_size parameter specified in global variables.\n",
        "# This step is only for model training pipeline (mtp).\n",
        "\n",
        "# Parameters for Train Test Split Step\n",
        "random_state = 42\n",
        "\n",
        "# Output variable name from this step\n",
        "step_output_data_name_train = 'train_data'\n",
        "step_output_data_name_test = 'test_data'\n",
        "step_display_title = \"Train_Test_Data_Split\"\n",
        "step_script_name = 'train_test_split.py'\n",
        "\n",
        "globals()[step_output_data_name_train] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_train, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_train}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name}{dataAssetName_suffix}',\n",
        "    # tags={'step':'output', 'train_data':'True', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "globals()[step_output_data_name_test] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_test, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_test}')\n",
        ").register_on_complete(\n",
        "    name = step_output_data_name_test, \n",
        "    # tags={'step':'output', 'train_data':'False', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "trainTestSplitStep = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path_train\", eval(step_output_data_name_train),\n",
        "        \"--output_data_path_test\", eval(step_output_data_name_test),\n",
        "        \"--test_size\", test_size,\n",
        "        \"--random_state\", random_state\n",
        "    ],\n",
        "    inputs=[\n",
        "        merged_with_churn_flag_data.read_parquet_files().as_input('merged_with_churn_flag_data'),\n",
        "    ],\n",
        "    outputs=[eval(step_output_data_name_train), eval(step_output_data_name_test)],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_preprocess_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.2.2] Outlier Treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will remove outliers based on outliers_threshold parameter specified in global variables.\n",
        "# This step is only for model training pipeline (mtp).\n",
        "\n",
        "# This step is Only for train data set\n",
        "outliers_method = 'iForest'\n",
        "outliers_threshold = 0.05\n",
        "\n",
        "step_output_data_name_train = 'outlier_treated_data_train'\n",
        "step_display_title = \"Outlier_Treatment\"\n",
        "step_script_name = 'outlier_treatment.py'\n",
        "pipelineStep = 'trainOutlierTreatmentStep'\n",
        "input_data_train = train_data\n",
        "\n",
        "globals()[step_output_data_name_train] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_train, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_train}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_train}{dataAssetName_suffix}',\n",
        "    # tags={'step':'output', 'train_data':'True', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "globals()[pipelineStep] = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path_train\", eval(step_output_data_name_train),\n",
        "        \"--outliers_threshold\", outliers_threshold, \n",
        "        \"--outliers_method\", outliers_method,\n",
        "        \"--njobs\", njobs,\n",
        "    ],\n",
        "    inputs=[\n",
        "        input_data_train.read_parquet_files().as_input('train_data'),\n",
        "    ],\n",
        "    outputs=[\n",
        "        eval(step_output_data_name_train), \n",
        "    ],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_preprocess_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")\n",
        "## This step should not be included for new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.2.3] Multicolinearity Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will remove columns that are highly correlated based on the multiCollinearity_threshold parameter specified in global variables.\n",
        "# This step is only for model training pipeline (mtp).\n",
        "\n",
        "multiCollinearity_method = 'pearson'\n",
        "multiCollinearity_threshold = 0.95\n",
        "\n",
        "step_output_data_name_train = 'multicollinearity_treated_data_train'\n",
        "step_output_data_name_test = 'multicollinearity_treated_data_test'\n",
        "step_display_title = \"Multi-Collinearity_Treatment\"\n",
        "step_script_name = 'multicollinearity_treatment.py'\n",
        "pipelineStep = 'multiCollinearityTreatmentStep'\n",
        "input_data_train = outlier_treated_data_train\n",
        "input_data_test = test_data\n",
        "\n",
        "globals()[step_output_data_name_train] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_train, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_train}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_train}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'train_data':'True', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "globals()[step_output_data_name_test] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_test, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_test}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_test}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'train_data':'False', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "globals()[pipelineStep] = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path_train\", eval(step_output_data_name_train),\n",
        "        \"--output_data_path_test\", eval(step_output_data_name_test),\n",
        "        \"--multiCollinearity_threshold\", multiCollinearity_threshold, \n",
        "        \"--multiCollinearity_method\", multiCollinearity_method,\n",
        "    ],\n",
        "    inputs=[\n",
        "        input_data_train.read_parquet_files().as_input('train_data'),\n",
        "        input_data_test.read_parquet_files().as_input('test_data'), \n",
        "    ],\n",
        "    outputs=[\n",
        "        eval(step_output_data_name_train), \n",
        "        eval(step_output_data_name_test)\n",
        "    ],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_preprocess_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.2.4] Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will select columns based on the featureSelection_method & featureSelection_threshold.\n",
        "# This step is only for model training pipeline (mtp).\n",
        "\n",
        "# feature selection method\n",
        "featureSelection_method = 'lightGBM'\n",
        "# Select features with score greater than or equal to featureSelection_threshold\n",
        "# featureSelection_threshold = 0 means select features based on featureSelection_percentage\n",
        "featureSelection_threshold = 0 #0.001\n",
        "# Select top x percetange of features x/100 = featureSelection_percentage\n",
        "# featureSelection_threshold will eliminate some features and then select \n",
        "# x (featureSelection_percentage) percentage of features from those selected features\n",
        "# featureSelection_percentage = 1 means select features based on featureSelection_threshold\n",
        "featureSelection_percentage = 1\n",
        "random_state = 42\n",
        "\n",
        "step_output_data_name_train = 'featureSelected_data_train'\n",
        "step_output_data_name_test = 'featureSelected_data_test'\n",
        "step_display_title = \"Feature Selection - Train Set\"\n",
        "step_script_name = 'feature_selection.py'\n",
        "pipelineStep = 'featureSelectionStep'\n",
        "input_data_train = multicollinearity_treated_data_train\n",
        "input_data_test = multicollinearity_treated_data_test\n",
        "\n",
        "globals()[step_output_data_name_train] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_train, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_train}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_train}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'train_data':'True', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "globals()[step_output_data_name_test] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_test, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_test}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_test}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'train_data':'False', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "globals()[pipelineStep] = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path_train\", eval(step_output_data_name_train),\n",
        "        \"--output_data_path_test\", eval(step_output_data_name_test),\n",
        "        \"--featureSelection_threshold\", featureSelection_threshold, \n",
        "        \"--featureSelection_percentage\", featureSelection_percentage, \n",
        "        \"--featureSelection_method\", featureSelection_method,\n",
        "        \"--random_state\", random_state,\n",
        "        \"--njobs\", njobs,\n",
        "    ],\n",
        "    inputs=[\n",
        "        input_data_train.read_parquet_files().as_input('train_data'),\n",
        "        input_data_test.read_parquet_files().as_input('test_data'), \n",
        "    ],\n",
        "    outputs=[\n",
        "        eval(step_output_data_name_train), \n",
        "        eval(step_output_data_name_test)\n",
        "    ],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_preprocess_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.2.5] Normalize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will normalize the data. For new data pipeline, train and test both will refer to same new data set.\n",
        "# This step is common for both model training pipeline (mtp) & new data pipeline (ndp).\n",
        "# Skip this manual normalization step: as azure will handle this for us automatically\n",
        "\n",
        "# normalization_method = 'standard'\n",
        "normalization_method = 'skip'\n",
        "\n",
        "# Formodel training pipeline (mtp)\n",
        "step_output_data_name_train = 'normalized_data_train'\n",
        "step_output_data_name_test = 'normalized_data_test'\n",
        "step_display_title = \"Data_Normalization\"\n",
        "step_script_name = 'data_normalization.py'\n",
        "pipelineStep = 'dataNormalizationStep'\n",
        "input_data_train = featureSelected_data_train\n",
        "input_data_test = featureSelected_data_test\n",
        "\n",
        "# For new data pipeline (ndp)\n",
        "if (new_data_flag == True) and (monitoring_flag == True):\n",
        "    print('Running Normalization Step for New Data Pipeline')\n",
        "    # step_output_data_name_test = 'normalized_data_new_data'\n",
        "    step_display_title = \"Data_Normalization-New_Data\"\n",
        "    # No splitting required for new data set. We set new data set as test set and run the code only for test set.\n",
        "    input_data_train = merged_with_churn_flag_data\n",
        "    input_data_test = merged_with_churn_flag_data\n",
        "elif (new_data_flag == True) and (monitoring_flag == False):\n",
        "    print('Running Normalization Step for New Data Pipeline')\n",
        "    # step_output_data_name_test = 'normalized_data_new_data'\n",
        "    step_display_title = \"Data_Normalization-New_Data\"\n",
        "    # No splitting required for new data set. We set new data set as test set and run the code only for test set.\n",
        "    input_data_train = merged_transformed_data\n",
        "    input_data_test = merged_transformed_data\n",
        "\n",
        "globals()[step_output_data_name_train] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_train, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_train}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_train}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'train_data':'True', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "globals()[step_output_data_name_test] = OutputFileDatasetConfig(\n",
        "    name = step_output_data_name_test, \n",
        "    destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_test}')\n",
        ").register_on_complete(\n",
        "    # data asset name on azure\n",
        "    name = f'{user_name}_{step_output_data_name_test}{dataAssetName_suffix}', \n",
        "    # tags={'step':'output', 'train_data':'False', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "    tags = pipeline_tags\n",
        ")\n",
        "\n",
        "globals()[pipelineStep] = PythonScriptStep(\n",
        "    name = step_display_title,\n",
        "    script_name=step_script_name,\n",
        "    arguments=[\n",
        "        \"--output_data_path_train\", eval(step_output_data_name_train),\n",
        "        \"--output_data_path_test\", eval(step_output_data_name_test), \n",
        "        \"--normalization_method\", normalization_method,\n",
        "        # \"--new_data_flag\", new_data_flag,\n",
        "    ],\n",
        "    inputs=[\n",
        "        input_data_train.read_parquet_files().as_input('train_data'),\n",
        "        input_data_test.read_parquet_files().as_input('test_data'), \n",
        "    ],\n",
        "    outputs=[\n",
        "        eval(step_output_data_name_train), \n",
        "        eval(step_output_data_name_test)\n",
        "    ],    \n",
        "    compute_target=aml_compute,\n",
        "    runconfig = aml_run_config,\n",
        "    source_directory=script_data_preprocess_folder,\n",
        "    allow_reuse = allow_reuse_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.2.6] Finalize Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [5.2.6.1] For Model Training Pipeline (MTP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will finalize the data for model training and testing. Step takes input from the previous component \n",
        "# i.e. either normalized_data or featureSelected_data\n",
        "# This step is only for model training pipeline (mtp).\n",
        "\n",
        "if not new_data_flag:\n",
        "    step_output_data_name_train = 'final_data_train'\n",
        "    step_output_data_name_test = 'final_data_test'\n",
        "    step_output_data_name_id_train = 'final_data_id_train'\n",
        "    step_output_data_name_id_test = 'final_data_id_test'\n",
        "\n",
        "    step_display_title = \"Finalize_Data-Train_Test\"\n",
        "    step_script_name = 'data_final_prep_mtp.py'\n",
        "    pipelineStep = 'finalDataPrepStep'\n",
        "    input_data_train = normalized_data_train\n",
        "    input_data_test = normalized_data_test\n",
        "    # input_data_train = featureSelected_data_train\n",
        "    # input_data_test = featureSelected_data_test\n",
        "\n",
        "    # Save final data sets\n",
        "    globals()[step_output_data_name_train] = OutputFileDatasetConfig(\n",
        "        name = step_output_data_name_train, \n",
        "        destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_train}')\n",
        "    ).register_on_complete(\n",
        "        # data asset name on azure\n",
        "        name = f'{user_name}_{step_output_data_name_train}{dataAssetName_suffix}', \n",
        "        # tags={'step':'output', 'train_data':'True', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "        tags = pipeline_tags\n",
        "    )\n",
        "    globals()[step_output_data_name_test] = OutputFileDatasetConfig(\n",
        "        name = step_output_data_name_test, \n",
        "        destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_test}')\n",
        "    ).register_on_complete(\n",
        "        # data asset name on azure\n",
        "        name = f'{user_name}_{step_output_data_name_test}{dataAssetName_suffix}', \n",
        "        # tags={'step':'output', 'train_data':'False', 'data': 'transformed', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "        tags = pipeline_tags\n",
        "    )\n",
        "\n",
        "    # For saving HASHED_CIF dataframe as a parquet file\n",
        "    globals()[step_output_data_name_id_train] = OutputFileDatasetConfig(\n",
        "        name = step_output_data_name_id_train, \n",
        "        destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_id_train}')\n",
        "    ).register_on_complete(\n",
        "        name = f'{user_name}_{step_output_data_name_id_train}{dataAssetName_suffix}',\n",
        "        # tags={\n",
        "        #     'step':'output', 'train_data':'True', 'data': 'transformed', \n",
        "        #     'sample_data':'false' ,'temporary_data': 'True', 'only_hashed_cif_id':'True'\n",
        "        # }\n",
        "        tags = pipeline_tags\n",
        "    )\n",
        "    globals()[step_output_data_name_id_test] = OutputFileDatasetConfig(\n",
        "        name = step_output_data_name_id_test, \n",
        "        destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_id_test}')\n",
        "    ).register_on_complete(\n",
        "        name = f'{user_name}_{step_output_data_name_id_test}{dataAssetName_suffix}',\n",
        "        # tags={\n",
        "        #     'step':'output', 'test_data':'True', 'data': 'transformed', \n",
        "        #     'sample_data':'false' ,'temporary_data': 'True', 'only_hashed_cif_id':'True'\n",
        "        # }\n",
        "        tags = pipeline_tags\n",
        "    )\n",
        "\n",
        "    globals()[pipelineStep] = PythonScriptStep(\n",
        "        name = step_display_title,\n",
        "        script_name=step_script_name,\n",
        "        arguments=[\n",
        "            \"--output_data_path_train\", eval(step_output_data_name_train),\n",
        "            \"--output_data_path_test\", eval(step_output_data_name_test), \n",
        "            \"--output_data_path_id_train\", eval(step_output_data_name_id_train),\n",
        "            \"--output_data_path_id_test\", eval(step_output_data_name_id_test), \n",
        "        ],\n",
        "        inputs=[\n",
        "            input_data_train.read_parquet_files().as_input('train_data'),\n",
        "            input_data_test.read_parquet_files().as_input('test_data'), \n",
        "        ],\n",
        "        outputs=[\n",
        "            eval(step_output_data_name_train), \n",
        "            eval(step_output_data_name_test),\n",
        "            eval(step_output_data_name_id_train), \n",
        "            eval(step_output_data_name_id_test),           \n",
        "        ],    \n",
        "        compute_target=aml_compute,\n",
        "        runconfig = aml_run_config,\n",
        "        source_directory=script_data_preprocess_folder,\n",
        "        allow_reuse = allow_reuse_step\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [5.2.6.2] For New Data Pipeline (NDP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step Overview:\n",
        "# Step will finalize the data for evaluation using the trained. This step will take input from the \n",
        "# merged_transformed_data, as no outlier, feature selection & multicolinearity needs to be done on new data.\n",
        "# These values will be taken from the training data.\n",
        "# This step is only for new data pipeline (ndp).\n",
        "\n",
        "if new_data_flag:\n",
        "    step_output_data_name_new_data = 'final_data_new_data'\n",
        "    step_output_data_name_id_new_data = 'final_data_id_new_data'\n",
        "\n",
        "    step_display_title = \"Finalize_Data-New_Data\"\n",
        "    step_script_name = 'data_final_prep_ndp.py'\n",
        "    pipelineStep = 'finalDataPrepStep'\n",
        "    # input_data_new_data = merged_transformed_data\n",
        "    input_data_new_data = normalized_data_test\n",
        "\n",
        "    globals()[step_output_data_name_new_data] = OutputFileDatasetConfig(\n",
        "        name = step_output_data_name_new_data, \n",
        "        destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_new_data}')\n",
        "    ).register_on_complete(\n",
        "        # data asset name on azure\n",
        "        name = f'{user_name}_{step_output_data_name_new_data}{dataAssetName_suffix}', \n",
        "        # tags={'step':'output', 'new_data':'True', 'data': 'transformed_new_data', 'sample_data':'false' ,'temporary_data': 'True'}\n",
        "        tags = pipeline_tags\n",
        "    )\n",
        "\n",
        "    # For saving HASHED_CIF dataframe as a parquet file\n",
        "    globals()[step_output_data_name_id_new_data] = OutputFileDatasetConfig(\n",
        "        name = step_output_data_name_id_new_data, \n",
        "        destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_id_new_data}')\n",
        "    ).register_on_complete(\n",
        "        name = f'{user_name}_{step_output_data_name_id_new_data}{dataAssetName_suffix}', \n",
        "        # tags={\n",
        "        #     'step':'output', 'new_data':'True', 'data': 'transformed_new_data', \n",
        "        #     'sample_data':'false' ,'temporary_data': 'True', 'only_hashed_cif_id':'True'\n",
        "        # }\n",
        "        tags = pipeline_tags\n",
        "    )\n",
        "\n",
        "    globals()[pipelineStep] = PythonScriptStep(\n",
        "        name = step_display_title,\n",
        "        script_name=step_script_name,\n",
        "        arguments=[\n",
        "            \"--output_data_path_new_data\", eval(step_output_data_name_new_data), \n",
        "            \"--output_data_path_id_new_data\", eval(step_output_data_name_id_new_data), \n",
        "            \"--monitoring_flag\", monitoring_flag\n",
        "        ],\n",
        "        inputs=[\n",
        "            input_data_new_data.read_parquet_files().as_input('new_data'),\n",
        "        ],\n",
        "        outputs=[\n",
        "            eval(step_output_data_name_new_data),  \n",
        "            eval(step_output_data_name_id_new_data)\n",
        "        ],    \n",
        "        compute_target=aml_compute,\n",
        "        runconfig = aml_run_config,\n",
        "        source_directory=script_data_preprocess_folder,\n",
        "        allow_reuse = allow_reuse_step\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [5.3] Model Building Pipeline Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.3.1] Train, Validate, HyperTune & Test Model Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define settings for autogeneration and tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is for new data flag, we use final_data_new_data for getting prediciton.\n",
        "# Model trainig will be skipped for NDP, Trained model from MTP will be used.\n",
        "if new_data_flag:\n",
        "    final_data_train = final_data_new_data\n",
        "    final_data_test = final_data_new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HyperParameter Tunning parameters, iterations: how many different models to be tried with differnt hyperparameters\n",
        "# More iterations => more different model algorithms and more different hyperparameters to be tried\n",
        "# comment n_cross_validations & validation_size to let azure ml optimize hyperparameter tunning and cross validation technique\n",
        "\n",
        "# iteration_timeout_minutes: Maximum time in minutes that each iteration can run for before it terminates. \n",
        "# If not specified, a value of 1 month or 43200 minutes is used.\n",
        "\n",
        "# iterations: The total number of different algorithm and hyper parameter combinations to test during an automated ML experiment. \n",
        "# If not specified, the default is 1000 iterations.\n",
        "\n",
        "# experiment_timeout_hours: Maximum amount of time in hours that all iterations combined can take before the experiment terminates. \n",
        "# Can be a decimal value like 0.25 representing 15 minutes. If not specified, the default experiment timeout is 6 days. \n",
        "# To specify a timeout less than or equal to 1 hour, make sure your dataset's size is not greater than 10,000,000 (rows times column) \n",
        "# or an error results.\n",
        "\n",
        "# primary_metric: The metric that Automated Machine Learning will optimize for model selection.\n",
        "# see https://docs.microsoft.com/azure/machine-learning/how-to-configure-auto-train#primary-metric.\n",
        "\n",
        "# max_concurrent_iterations: Represents the maximum number of iterations that would be executed in parallel. The default value is 1.\n",
        "\n",
        "# enable_early_stopping: Whether to enable early termination if the score is not improving in the short term. The default is True.\n",
        "\n",
        "# n_cross_validations: How many cross validations to perform when user validation data is not specified.\n",
        "\n",
        "# validation_size: What fraction of the data to hold out for validation when user validation data is not specified. \n",
        "# This should be between 0.0 and 1.0 non-inclusive.\n",
        "\n",
        "\n",
        "automl_settings = {\n",
        "    \"iteration_timeout_minutes\" : 10,\n",
        "    \"iterations\" : model_iterations,\n",
        "    \"experiment_timeout_hours\" : experiment_timeout_hours,\n",
        "    \"primary_metric\" : primary_metric,\n",
        "    \"max_concurrent_iterations\" : num_max_nodes,\n",
        "    \"enable_early_stopping\" : True,\n",
        "    \"n_cross_validations\" : 5, \n",
        "    \"validation_size\" : 0.1,\n",
        "}\n",
        "\n",
        "# ensemble_settings = {\n",
        "#     \"ensemble_download_models_timeout_sec\": 600,\n",
        "#     \"stack_meta_learner_type\": \"LogisticRegressionCV\",\n",
        "#     \"stack_meta_learner_train_percentage\": 0.3,\n",
        "#     \"stack_meta_learner_kwargs\": {\n",
        "#                                 \"refit\": True,\n",
        "#                                 \"fit_intercept\": False,\n",
        "#                                 \"class_weight\": \"balanced\",\n",
        "#                                 \"multi_class\": \"auto\",\n",
        "#                                 \"n_jobs\": -1\n",
        "#                                 }\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AutoML config created.\n"
          ]
        }
      ],
      "source": [
        "# Converting data type from OutputFileDatasetConfig to PipelineData using .read_parquet_files(),\n",
        "# as AutoMLConfig only takes input data type as PipelineData\n",
        "automl_config = AutoMLConfig(\n",
        "    task = 'classification',\n",
        "    debug_log = 'automated_ml_errors.log',\n",
        "    path = script_modeling_folder,\n",
        "    compute_target = aml_compute,\n",
        "    featurization = 'auto',\n",
        "    training_data = final_data_train.read_parquet_files(),\n",
        "    label_column_name = 'churn_flag',\n",
        "    test_data = final_data_test.read_parquet_files(),\n",
        "    **automl_settings,\n",
        "    # **ensemble_settings\n",
        ")\n",
        "                            \n",
        "print(\"AutoML config created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specify automated ML outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_data = PipelineData(\n",
        "    name='metrics_data',\n",
        "    datastore=default_store,\n",
        "    pipeline_output_name='metrics_output',\n",
        "    training_output=TrainingOutput(type='Metrics')\n",
        ")\n",
        "\n",
        "model_data = PipelineData(\n",
        "    name='best_model_data',\n",
        "    datastore=default_store,\n",
        "    pipeline_output_name='model_output',\n",
        "    training_output=TrainingOutput(type='Model')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainWithAutomlStep created.\n"
          ]
        }
      ],
      "source": [
        "step_display_title = 'Model_Training_Testing'\n",
        "\n",
        "trainWithAutomlStep = AutoMLStep(\n",
        "    name=step_display_title,\n",
        "    automl_config=automl_config,\n",
        "    passthru_automl_config=False,\n",
        "    outputs=[\n",
        "        metrics_data, model_data\n",
        "    ],\n",
        "    enable_default_model_output=False,\n",
        "    enable_default_metrics_output=False,\n",
        "    allow_reuse=allow_reuse_step\n",
        ")\n",
        "print(\"trainWithAutomlStep created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.3.2] Registering Model Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registering Model Step created.\n"
          ]
        }
      ],
      "source": [
        "# The model name with which to register the trained model in the workspace.\n",
        "step_display_title = \"Register_Model\"\n",
        "\n",
        "# For adding all tags to registered model\n",
        "pipeline_tags.update(automl_settings)\n",
        "pipeline_tags_model = pipeline_tags.copy()\n",
        "for key in pipeline_parameters.keys():\n",
        "    pipeline_tags_model.pop(key, None)\n",
        "\n",
        "registerModelStep = PythonScriptStep(\n",
        "script_name=\"register_model.py\",\n",
        "    name=step_display_title,\n",
        "    arguments=[\n",
        "        \"--model_name\", pp_model_name, \n",
        "        \"--model_path\", model_data,\n",
        "        \"--observation_year\", pp_observation_year,\n",
        "        \"--observation_month_number\", pp_observation_month_number,\n",
        "        \"--historical_months\", pp_historical_months,\n",
        "        \"--latest_data_ndp_flag\", pp_latest_data_ndp_flag,\n",
        "        \"--model_tags\", str(pipeline_tags_model),\n",
        "        \"--user_selected_metric\", user_selected_metric,\n",
        "        \"--experiment_name\", experiment_name,\n",
        "        \"--ws_details\", ws_details,\n",
        "    ],\n",
        "    inputs=[\n",
        "        model_data, \n",
        "        final_data_train.read_parquet_files().as_input('train_data'),\n",
        "    ],\n",
        "    compute_target=aml_compute,\n",
        "    runconfig=aml_run_config,\n",
        "    source_directory=script_modeling_folder,\n",
        "    allow_reuse=allow_reuse_step,\n",
        ")\n",
        "print(\"Registering Model Step created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.3.3] Model Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [5.3.2.1] Model Training Pipeline (MTP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not new_data_flag:\n",
        "    step_output_data_name_train = 'predicted_data_train'\n",
        "    step_output_data_name_test = 'predicted_data_test'\n",
        "\n",
        "    step_display_title = \"Prediction_Results-Train_Test\"\n",
        "    step_script_name = 'get_predictions_mtp.py'\n",
        "    pipelineStep = 'modelPredictionStep'\n",
        "\n",
        "    # Save model predictions\n",
        "    globals()[step_output_data_name_train] = OutputFileDatasetConfig(\n",
        "        name = step_output_data_name_train, \n",
        "        destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_train}')\n",
        "    ).register_on_complete(\n",
        "        # data asset name on azure\n",
        "        name = f'{user_name}_{step_output_data_name_train}{dataAssetName_suffix}', \n",
        "        # tags={'step':'output', 'train_data':'True', 'data': 'transformed', 'sample_data':'false' ,\n",
        "        # 'temporary_data': 'False', 'output_file_type' : 'csv'}\n",
        "        tags = pipeline_tags\n",
        "    )\n",
        "    globals()[step_output_data_name_test] = OutputFileDatasetConfig(\n",
        "        name = step_output_data_name_test, \n",
        "        destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_test}')\n",
        "    ).register_on_complete(\n",
        "        # data asset name on azure\n",
        "        name = f'{user_name}_{step_output_data_name_test}{dataAssetName_suffix}', \n",
        "        # tags={'step':'output', 'train_data':'False', 'data': 'transformed', 'sample_data':'false' ,\n",
        "        # 'temporary_data': 'False','output_file_type' : 'csv'}\n",
        "        tags = pipeline_tags\n",
        "    )\n",
        "\n",
        "    globals()[pipelineStep] = PythonScriptStep(\n",
        "        name = step_display_title,\n",
        "        script_name=step_script_name,\n",
        "        arguments=[\n",
        "            \"--output_data_path_train\", eval(step_output_data_name_train),\n",
        "            \"--output_data_path_test\", eval(step_output_data_name_test), \n",
        "            \"--experiment_name\", experiment_name\n",
        "        ],\n",
        "        inputs=[\n",
        "            final_data_train.read_parquet_files().as_input('train_data'),\n",
        "            final_data_test.read_parquet_files().as_input('test_data'), \n",
        "            final_data_id_train.read_parquet_files().as_input('train_data_id'),\n",
        "            final_data_id_test.read_parquet_files().as_input('test_data_id'), \n",
        "            model_data\n",
        "        ],\n",
        "        outputs=[\n",
        "            eval(step_output_data_name_train), \n",
        "            eval(step_output_data_name_test),\n",
        "        ],    \n",
        "        compute_target=aml_compute,\n",
        "        runconfig = aml_run_config,\n",
        "        source_directory=script_modeling_folder,\n",
        "        allow_reuse = allow_reuse_step\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [5.3.2.2] New Data Pipeline (NDP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "if new_data_flag:\n",
        "    step_output_data_name_new_data = 'predicted_data_new_data'\n",
        "\n",
        "    step_display_title = \"Prediction_Results-New_Data\"\n",
        "    step_script_name = 'get_predictions_ndp.py'\n",
        "    pipelineStep = 'modelPredictionStep'\n",
        "\n",
        "    globals()[step_output_data_name_new_data] = OutputFileDatasetConfig(\n",
        "        name = step_output_data_name_new_data, \n",
        "        destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_new_data}')\n",
        "    ).register_on_complete(\n",
        "        # data asset name on azure\n",
        "        name = f'{user_name}_{step_output_data_name_new_data}{dataAssetName_suffix}', \n",
        "        # tags={'step':'output', 'new_data':'True', 'data': 'transformed_new_data', 'sample_data':'false',\n",
        "        # 'temporary_data': 'False','output_file_type' : 'csv'}\n",
        "        tags = pipeline_tags\n",
        "    )\n",
        "\n",
        "    globals()[pipelineStep] = PythonScriptStep(\n",
        "        name = step_display_title,\n",
        "        script_name=step_script_name,\n",
        "        arguments=[\n",
        "            \"--output_data_path_new_data\", eval(step_output_data_name_new_data),\n",
        "            \"--model_name\", model_name,\n",
        "            \"--model_version\", pp_model_version_ndp,\n",
        "            # \"--model_path\",  model_data,\n",
        "            \"--experiment_name\", experiment_name,\n",
        "        ],\n",
        "        inputs=[\n",
        "            final_data_new_data.read_parquet_files().as_input('new_data'), \n",
        "            final_data_id_new_data.read_parquet_files().as_input('new_data_id'),\n",
        "            # model_data\n",
        "        ],\n",
        "        outputs=[\n",
        "            eval(step_output_data_name_new_data)\n",
        "        ],    \n",
        "        compute_target=aml_compute,\n",
        "        runconfig = aml_run_config,\n",
        "        source_directory=script_modeling_folder,\n",
        "        allow_reuse = allow_reuse_step\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "if new_data_flag:\n",
        "    # step_output_data_name_new_data = 'predicted_data_new_data'\n",
        "\n",
        "    step_display_title = \"Model_Monitoring-New_Data\"\n",
        "    step_script_name = 'model_monitoring.py'\n",
        "    pipelineStep = 'modelMonitoringStep'\n",
        "\n",
        "    # globals()[step_output_data_name_new_data] = OutputFileDatasetConfig(\n",
        "    #     name = step_output_data_name_new_data, \n",
        "    #     destination = (default_store, f'{output_data_intermediate_path}/{step_output_data_name_new_data}')\n",
        "    # ).register_on_complete(\n",
        "    #     # data asset name on azure\n",
        "    #     name = f'{user_name}_{step_output_data_name_new_data}{dataAssetName_suffix}', \n",
        "    #     # tags={'step':'output', 'new_data':'True', 'data': 'transformed_new_data', 'sample_data':'false',\n",
        "    #     # 'temporary_data': 'False','output_file_type' : 'csv'}\n",
        "    #     tags = pipeline_tags\n",
        "    # )\n",
        "\n",
        "    globals()[pipelineStep] = PythonScriptStep(\n",
        "        name = step_display_title,\n",
        "        script_name=step_script_name,\n",
        "        arguments=[\n",
        "            # \"--output_data_path_new_data\", eval(step_output_data_name_new_data),\n",
        "            \"--model_name\", model_name,\n",
        "            \"--model_version\", pp_model_version_ndp,\n",
        "            # \"--model_path\",  model_data,\n",
        "            \"--experiment_name\", experiment_name,\n",
        "            \"--user_selected_metric\", user_selected_metric\n",
        "        ],\n",
        "        inputs=[\n",
        "            predicted_data_new_data.read_delimited_files().as_input('model_pred_data'), \n",
        "            # final_data_id_new_data.read_parquet_files().as_input('new_data_id'),\n",
        "            # model_data\n",
        "        ],\n",
        "        # outputs=[\n",
        "        #     eval(step_output_data_name_new_data)\n",
        "        # ],    \n",
        "        compute_target=aml_compute,\n",
        "        runconfig = aml_run_config,\n",
        "        source_directory=script_modeling_folder,\n",
        "        allow_reuse = allow_reuse_step\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [5.4] Running Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.4.1] Creating & Validating Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### [5.4.1.1] For Model Training Pipeline (MTP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Transformation Steps\n",
        "pipeline_steps_data_transformation = [\n",
        "    fetchLatestDataStep,\n",
        "    transformingDemographicsStep, \n",
        "    transformingTransactionStep, \n",
        "    transformingAccountStep, \n",
        "    featureEngTransactionStep,\n",
        "    mergingTransformedDataStep,\n",
        "    creatingChurnFlagStep,\n",
        "    mergedWithChurnStep\n",
        "]\n",
        "\n",
        "# if sample flag is true, then we run the sampling step in the pipeline else we skip\n",
        "if sample_flag:\n",
        "    pipeline_steps_data_transformation = [samplingDataStep] + pipeline_steps_data_transformation\n",
        "    print('Pipeline will be created with step newSamplingDataStep as sample_flag is set to True')\n",
        "\n",
        "# Data Preprocess Steps\n",
        "pipeline_steps_data_preprocess = [\n",
        "    trainTestSplitStep,\n",
        "    trainOutlierTreatmentStep,\n",
        "    multiCollinearityTreatmentStep,\n",
        "    featureSelectionStep,\n",
        "    dataNormalizationStep,\n",
        "    finalDataPrepStep\n",
        "]\n",
        "# Modeling Steps\n",
        "pipeline_steps_modeling = [\n",
        "    trainWithAutomlStep, \n",
        "    registerModelStep,\n",
        "    modelPredictionStep\n",
        "]\n",
        "\n",
        "# Creating single list of all pipeline steps\n",
        "train_test_pipeline_steps = pipeline_steps_data_transformation + pipeline_steps_data_preprocess + pipeline_steps_modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### [5.4.1.2] For New Data Pipeline (NDP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Transformation Steps\n",
        "pipeline_steps_data_transformation = [\n",
        "    fetchLatestDataStep,\n",
        "    transformingDemographicsStep, \n",
        "    transformingTransactionStep, \n",
        "    transformingAccountStep, \n",
        "    featureEngTransactionStep,\n",
        "    mergingTransformedDataStep,\n",
        "]\n",
        "\n",
        "# if sample flag is true, then we run the sampling step in the pipeline else we skip\n",
        "if sample_flag:\n",
        "    pipeline_steps_data_transformation = [samplingDataStep] + pipeline_steps_data_transformation\n",
        "    print('Pipeline will be created with step newSamplingDataStep as sample_flag is set to True')\n",
        "\n",
        "if monitoring_flag == True and new_data_flag == True:\n",
        "    pipeline_steps_data_transformation = [creatingChurnFlagStep, mergedWithChurnStep] + pipeline_steps_data_transformation \n",
        "\n",
        "# Data Preprocess Steps\n",
        "pipeline_steps_data_preprocess = [\n",
        "    # dataNormalizationStep,\n",
        "    finalDataPrepStep,\n",
        "    modelPredictionStep,\n",
        "    # modelMonitoringStep\n",
        "]\n",
        "\n",
        "if monitoring_flag == True and new_data_flag == True:\n",
        "    pipeline_steps_data_preprocess = [modelMonitoringStep] + pipeline_steps_data_preprocess\n",
        "\n",
        "# Creating single list of all pipeline steps\n",
        "new_data_pipeline_steps = pipeline_steps_data_transformation + pipeline_steps_data_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1691220527130
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "InProgress.\n",
            "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n",
            "Pipeline master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00 is built\n",
            "Step Model_Training_Testing is ready to be created [8022d41d]\n",
            "Step Register_Model is ready to be created [4400684b]\n",
            "Step Prediction_Results-Train_Test is ready to be created [c045c699]\n",
            "Simple validation complete\n",
            "CPU times: user 1.19 s, sys: 113 ms, total: 1.3 s\n",
            "Wall time: 9.38 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Check the cluster status\n",
        "aml_compute.wait_for_completion(show_output=True)\n",
        "\n",
        "# Defining pipeline\n",
        "if new_data_flag:\n",
        "    # For NDP - new data pipeline\n",
        "    pipeline = Pipeline(ws, steps = new_data_pipeline_steps, description = f'Pipeline: {pipeline_name}')\n",
        "else:\n",
        "    # For MTP - model training pipeline\n",
        "    pipeline = Pipeline(ws, steps = train_test_pipeline_steps, description = f'Pipeline: {pipeline_name}')\n",
        "print(f'Pipeline {pipeline_name} is built')\n",
        "\n",
        "# Validate the pipeline\n",
        "pipeline.validate()\n",
        "print(\"Simple validation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.4.2] Creating & Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "gather": {
          "logged": 1691220531769
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment created master_model_training_pipeline\n",
            "Created step Fetch_Latest_Data [ba45bd62][51b5569e-db68-4068-bd08-82cae6f99571], (This step is eligible to reuse a previous run's output)\n",
            "Created step Transform_Demographics_Data [5837c670][ef00461e-0ca2-4720-8d37-f135922ccbe0], (This step is eligible to reuse a previous run's output)\n",
            "Created step Transform_Transaction_Data [47be9d02][e5082507-8c09-47ff-b969-60e69f83e5cd], (This step is eligible to reuse a previous run's output)\n",
            "Created step Transform_Account_Data [f9c3f177][f242db27-3d7d-43eb-b328-9831c01e16af], (This step is eligible to reuse a previous run's output)\n",
            "Created step Feature_Engineering-Transaction_Data [6e158514][fa2e3dcc-1087-4b96-ae51-95f2ae1d5f6a], (This step is eligible to reuse a previous run's output)\n",
            "Created step Merge_Transformed_Data [603bdfea][e1aa6cb8-dbf6-4c06-9f11-1e79e1f23996], (This step is eligible to reuse a previous run's output)\n",
            "Created step Create_Target-Churn_Flag [b6ce90e2][7e0454d4-169b-45ec-951b-aca09da5a78c], (This step is eligible to reuse a previous run's output)\n",
            "Created step Merge_Data_with_Churn_Flag [c2dda79a][6ecfbe61-7c41-4d2c-beff-3c2dc19e1f16], (This step is eligible to reuse a previous run's output)\n",
            "Created step Train_Test_Data_Split [c83f1880][52eb6d94-8aa6-4522-aa8c-227b8e829f94], (This step is eligible to reuse a previous run's output)\n",
            "Created step Outlier_Treatment [856ee381][ee2e87f1-a102-4ffd-a35b-1b406538f201], (This step is eligible to reuse a previous run's output)\n",
            "Created step Multi-Collinearity_Treatment [4156011d][8a52b0c5-f208-4be8-a025-fbc421c54dd5], (This step is eligible to reuse a previous run's output)\n",
            "Created step Feature Selection - Train Set [8afc43f6][be722a59-4e19-4b37-9acb-72f5708837f6], (This step is eligible to reuse a previous run's output)\n",
            "Created step Data_Normalization [b23fef29][1f08bf29-70da-4c88-846d-df56e1d89c7b], (This step is eligible to reuse a previous run's output)\n",
            "Created step Finalize_Data-Train_Test [936f05ac][e9b08aed-9f71-4a60-8c10-13f1ecc48d3e], (This step is eligible to reuse a previous run's output)\n",
            "Created step Model_Training_Testing [8022d41d][d08e1353-094e-4b12-9ce1-dc6f202ba6f1], (This step will run and generate new outputs)\n",
            "Created step Register_Model [4400684b][f9ab0eee-3f55-4de6-861f-02d447a64463], (This step will run and generate new outputs)\n",
            "Created step Prediction_Results-Train_Test [c045c699][d69efbd5-df2e-406c-86da-f42a0903210b], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (model_version: -1)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (new_data_flag: False)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (schedule_pipeline_flag: False)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (latest_data_ndp_flag: False)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (sample_flag: False)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (sample_size: 10000)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (reuse_sample_flag: False)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (observation_month_number: PipelineParameter_Name:observation_month_number_Default:10)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (observation_year: PipelineParameter_Name:observation_year_Default:2022)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (historical_months: PipelineParameter_Name:historical_months_Default:6)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (allow_reuse_step: True)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (regenerate_outputs: False)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (num_min_nodes: 0)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (num_max_nodes: 10)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (test_size: 0.1)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (outliers_threshold: 0.05)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (multiCollinearity_threshold: 0.95)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (featureSelection_threshold: 0)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (featureSelection_percentage: 1)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (iteration_timeout_minutes: 10)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (iterations: 40)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (experiment_timeout_hours: 3)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (max_concurrent_iterations: 10)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (enable_early_stopping: True)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (n_cross_validations: 5)\n",
            "WARNING:azureml.PipelineRun#d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c.RunHistoryFacade.RunClient:Converting non-string tag to string: (validation_size: 0.1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c?wsid=/subscriptions/bdea9b80-e147-4dd1-9e22-afd1228b6d1a/resourcegroups/casa-churn-analysis-ey-demo/workspaces/AzureConnection&tid=55587992-a704-412d-a52c-ce2a96ed3cf4\n",
            "Pipeline submitted for execution\n",
            "CPU times: user 1.18 s, sys: 161 ms, total: 1.34 s\n",
            "Wall time: 4.46 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if not schedule_pipeline_flag:\n",
        "    experiment = Experiment(ws, experiment_name)\n",
        "    print(f'Experiment created {experiment_name}')\n",
        "\n",
        "### regenerate_outputs Flag\n",
        "### If regenerate_outputs is set to True for the Experiment.Submit() call, a new submit will always force generation of all step outputs, \n",
        "### and disallow data reuse for any step of this run. Once this run is complete, however, subsequent runs may reuse the results of this run. \n",
        "### Default behavior of Pipelines is to set regenerate_outputs=False for experiment submit calls.\n",
        "if not schedule_pipeline_flag:\n",
        "    pipeline_run = experiment.submit(\n",
        "        pipeline, regenerate_outputs=regenerate_outputs, name=pipeline_name, \n",
        "        tags=pipeline_tags, pipeline_parameters=pipeline_parameters,\n",
        "    )\n",
        "    print('Pipeline submitted for execution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineRunId: d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/d0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c?wsid=/subscriptions/bdea9b80-e147-4dd1-9e22-afd1228b6d1a/resourcegroups/casa-churn-analysis-ey-demo/workspaces/AzureConnection&tid=55587992-a704-412d-a52c-ce2a96ed3cf4\n",
            "PipelineRun Status: NotStarted\n",
            "PipelineRun Status: Running\n",
            "\n",
            "\n",
            "StepRunId: 1630a665-6cc8-482e-ab69-2f33e1a2e993\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/1630a665-6cc8-482e-ab69-2f33e1a2e993?wsid=/subscriptions/bdea9b80-e147-4dd1-9e22-afd1228b6d1a/resourcegroups/casa-churn-analysis-ey-demo/workspaces/AzureConnection&tid=55587992-a704-412d-a52c-ce2a96ed3cf4\n",
            "StepRun( Fetch_Latest_Data ) Status: NotStarted\n",
            "StepRun( Fetch_Latest_Data ) Status: Running\n",
            "\n",
            "StepRun(Fetch_Latest_Data) Execution Summary\n",
            "=============================================\n",
            "StepRun( Fetch_Latest_Data ) Status: Finished\n",
            "{'runId': '1630a665-6cc8-482e-ab69-2f33e1a2e993', 'target': 'master-cpu-cluster-mtp', 'status': 'Completed', 'startTimeUtc': '2023-09-20T07:57:24.811421Z', 'endTimeUtc': '2023-09-20T08:00:46.683951Z', 'services': {}, 'properties': {'ContentSnapshotId': 'a61a6f29-b20a-4817-b44d-eeff13a86e67', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '51b5569e-db68-4068-bd08-82cae6f99571', 'azureml.moduleName': 'Fetch_Latest_Data', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': 'ba45bd62', 'azureml.pipelinerunid': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipeline': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.rootpipelinerunid': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlctrain', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'outputDatasets': [{'identifier': {'savedId': '1f5b57ef-753e-4457-9374-9a14aa336ca9', 'registeredId': 'cb09cf55-5f5e-4164-bf44-839938e91749', 'registeredVersion': '3'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'transaction_data'}, 'dataset': {\n",
            "  \"source\": [\n",
            "    \"('bidv_blob_datastore', 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/transaction_data')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"1f5b57ef-753e-4457-9374-9a14aa336ca9\",\n",
            "    \"name\": \"master_transaction_data_mtp\",\n",
            "    \"version\": 3,\n",
            "    \"description\": \"transaction_data for PipelineParameter_Name:observation_year_Default:2022_PipelineParameter_Name:observation_month_number_Default:10\",\n",
            "    \"tags\": {\n",
            "      \"pipeline type\": \"Model Training Pipeline (MTP)\",\n",
            "      \"model_version\": \"-1\",\n",
            "      \"pipeline_name\": \"master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00\",\n",
            "      \"username\": \"master\",\n",
            "      \"output_data_intermediate_path\": \"Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00\",\n",
            "      \"new_data_flag\": \"False\",\n",
            "      \"schedule_pipeline_flag\": \"False\",\n",
            "      \"latest_data_ndp_flag\": \"False\",\n",
            "      \"data_path_to_monitor\": \"InputData/InputRawFiles/transaction_data/\",\n",
            "      \"sample_flag\": \"False\",\n",
            "      \"sample_size\": \"10000\",\n",
            "      \"reuse_sample_flag\": \"False\",\n",
            "      \"observation_month_number\": \"PipelineParameter_Name:observation_month_number_Default:10\",\n",
            "      \"observation_year\": \"PipelineParameter_Name:observation_year_Default:2022\",\n",
            "      \"historical_months\": \"PipelineParameter_Name:historical_months_Default:6\",\n",
            "      \"allow_reuse_step\": \"True\",\n",
            "      \"regenerate_outputs\": \"False\",\n",
            "      \"amlcompute_cluster_name\": \"master-cpu-cluster-mtp\",\n",
            "      \"num_min_nodes\": \"0\",\n",
            "      \"num_max_nodes\": \"10\",\n",
            "      \"test_size\": \"0.1\",\n",
            "      \"outliers_method\": \"iForest\",\n",
            "      \"outliers_threshold\": \"0.05\",\n",
            "      \"multiCollinearity_method\": \"pearson\",\n",
            "      \"multiCollinearity_threshold\": \"0.95\",\n",
            "      \"featureSelection_method\": \"lightGBM\",\n",
            "      \"featureSelection_threshold\": \"0\",\n",
            "      \"featureSelection_percentage\": \"1\",\n",
            "      \"normalization_method\": \"skip\",\n",
            "      \"model_name\": \"master_ChurnPrediction_Model\",\n",
            "      \"primary_metric\": \"norm_macro_recall\",\n",
            "      \"user_selected_metric\": \"\",\n",
            "      \"iteration_timeout_minutes\": \"10\",\n",
            "      \"iterations\": \"40\",\n",
            "      \"experiment_timeout_hours\": \"3\",\n",
            "      \"max_concurrent_iterations\": \"10\",\n",
            "      \"enable_early_stopping\": \"True\",\n",
            "      \"n_cross_validations\": \"5\",\n",
            "      \"validation_size\": \"0.1\"\n",
            "    },\n",
            "    \"workspace\": \"Workspace.create(name='AzureConnection', subscription_id='bdea9b80-e147-4dd1-9e22-afd1228b6d1a', resource_group='casa-churn-analysis-ey-demo')\"\n",
            "  }\n",
            "}}, {'identifier': {'savedId': '40398c39-ee3e-47f5-baa9-cdea4214fd40', 'registeredId': '57d989e2-f503-4d2d-8f97-a4aa844130ca', 'registeredVersion': '3'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'demographic_data'}, 'dataset': {\n",
            "  \"source\": [\n",
            "    \"('bidv_blob_datastore', 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/demographic_data')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"40398c39-ee3e-47f5-baa9-cdea4214fd40\",\n",
            "    \"name\": \"master_demographic_data_mtp\",\n",
            "    \"version\": 3,\n",
            "    \"tags\": {\n",
            "      \"pipeline type\": \"Model Training Pipeline (MTP)\",\n",
            "      \"model_version\": \"-1\",\n",
            "      \"pipeline_name\": \"master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00\",\n",
            "      \"username\": \"master\",\n",
            "      \"output_data_intermediate_path\": \"Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00\",\n",
            "      \"new_data_flag\": \"False\",\n",
            "      \"schedule_pipeline_flag\": \"False\",\n",
            "      \"latest_data_ndp_flag\": \"False\",\n",
            "      \"data_path_to_monitor\": \"InputData/InputRawFiles/transaction_data/\",\n",
            "      \"sample_flag\": \"False\",\n",
            "      \"sample_size\": \"10000\",\n",
            "      \"reuse_sample_flag\": \"False\",\n",
            "      \"observation_month_number\": \"PipelineParameter_Name:observation_month_number_Default:10\",\n",
            "      \"observation_year\": \"PipelineParameter_Name:observation_year_Default:2022\",\n",
            "      \"historical_months\": \"PipelineParameter_Name:historical_months_Default:6\",\n",
            "      \"allow_reuse_step\": \"True\",\n",
            "      \"regenerate_outputs\": \"False\",\n",
            "      \"amlcompute_cluster_name\": \"master-cpu-cluster-mtp\",\n",
            "      \"num_min_nodes\": \"0\",\n",
            "      \"num_max_nodes\": \"10\",\n",
            "      \"test_size\": \"0.1\",\n",
            "      \"outliers_method\": \"iForest\",\n",
            "      \"outliers_threshold\": \"0.05\",\n",
            "      \"multiCollinearity_method\": \"pearson\",\n",
            "      \"multiCollinearity_threshold\": \"0.95\",\n",
            "      \"featureSelection_method\": \"lightGBM\",\n",
            "      \"featureSelection_threshold\": \"0\",\n",
            "      \"featureSelection_percentage\": \"1\",\n",
            "      \"normalization_method\": \"skip\",\n",
            "      \"model_name\": \"master_ChurnPrediction_Model\",\n",
            "      \"primary_metric\": \"norm_macro_recall\",\n",
            "      \"user_selected_metric\": \"\",\n",
            "      \"iteration_timeout_minutes\": \"10\",\n",
            "      \"iterations\": \"40\",\n",
            "      \"experiment_timeout_hours\": \"3\",\n",
            "      \"max_concurrent_iterations\": \"10\",\n",
            "      \"enable_early_stopping\": \"True\",\n",
            "      \"n_cross_validations\": \"5\",\n",
            "      \"validation_size\": \"0.1\"\n",
            "    },\n",
            "    \"workspace\": \"Workspace.create(name='AzureConnection', subscription_id='bdea9b80-e147-4dd1-9e22-afd1228b6d1a', resource_group='casa-churn-analysis-ey-demo')\"\n",
            "  }\n",
            "}}, {'identifier': {'savedId': '360996f4-9207-479d-925e-9cb66bd7bad4', 'registeredId': 'a132bb13-aba9-4780-9df0-d39307aed890', 'registeredVersion': '3'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'account_data'}, 'dataset': {\n",
            "  \"source\": [\n",
            "    \"('bidv_blob_datastore', 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/account_data')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"360996f4-9207-479d-925e-9cb66bd7bad4\",\n",
            "    \"name\": \"master_account_data_mtp\",\n",
            "    \"version\": 3,\n",
            "    \"tags\": {\n",
            "      \"pipeline type\": \"Model Training Pipeline (MTP)\",\n",
            "      \"model_version\": \"-1\",\n",
            "      \"pipeline_name\": \"master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00\",\n",
            "      \"username\": \"master\",\n",
            "      \"output_data_intermediate_path\": \"Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00\",\n",
            "      \"new_data_flag\": \"False\",\n",
            "      \"schedule_pipeline_flag\": \"False\",\n",
            "      \"latest_data_ndp_flag\": \"False\",\n",
            "      \"data_path_to_monitor\": \"InputData/InputRawFiles/transaction_data/\",\n",
            "      \"sample_flag\": \"False\",\n",
            "      \"sample_size\": \"10000\",\n",
            "      \"reuse_sample_flag\": \"False\",\n",
            "      \"observation_month_number\": \"PipelineParameter_Name:observation_month_number_Default:10\",\n",
            "      \"observation_year\": \"PipelineParameter_Name:observation_year_Default:2022\",\n",
            "      \"historical_months\": \"PipelineParameter_Name:historical_months_Default:6\",\n",
            "      \"allow_reuse_step\": \"True\",\n",
            "      \"regenerate_outputs\": \"False\",\n",
            "      \"amlcompute_cluster_name\": \"master-cpu-cluster-mtp\",\n",
            "      \"num_min_nodes\": \"0\",\n",
            "      \"num_max_nodes\": \"10\",\n",
            "      \"test_size\": \"0.1\",\n",
            "      \"outliers_method\": \"iForest\",\n",
            "      \"outliers_threshold\": \"0.05\",\n",
            "      \"multiCollinearity_method\": \"pearson\",\n",
            "      \"multiCollinearity_threshold\": \"0.95\",\n",
            "      \"featureSelection_method\": \"lightGBM\",\n",
            "      \"featureSelection_threshold\": \"0\",\n",
            "      \"featureSelection_percentage\": \"1\",\n",
            "      \"normalization_method\": \"skip\",\n",
            "      \"model_name\": \"master_ChurnPrediction_Model\",\n",
            "      \"primary_metric\": \"norm_macro_recall\",\n",
            "      \"user_selected_metric\": \"\",\n",
            "      \"iteration_timeout_minutes\": \"10\",\n",
            "      \"iterations\": \"40\",\n",
            "      \"experiment_timeout_hours\": \"3\",\n",
            "      \"max_concurrent_iterations\": \"10\",\n",
            "      \"enable_early_stopping\": \"True\",\n",
            "      \"n_cross_validations\": \"5\",\n",
            "      \"validation_size\": \"0.1\"\n",
            "    },\n",
            "    \"workspace\": \"Workspace.create(name='AzureConnection', subscription_id='bdea9b80-e147-4dd1-9e22-afd1228b6d1a', resource_group='casa-churn-analysis-ey-demo')\"\n",
            "  }\n",
            "}}], 'runDefinition': {'script': 'fetch_latest_data.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--output_data_path_demo', 'DatasetOutputConfig:demographic_data', '--output_data_path_trx', 'DatasetOutputConfig:transaction_data', '--output_data_path_acc', 'DatasetOutputConfig:account_data', '--observation_year', '$AML_PARAMETER_observation_year', '--observation_month_number', '$AML_PARAMETER_observation_month_number', '--historical_months', '$AML_PARAMETER_historical_months', '--input_raw_data_path', 'InputData/InputRawFiles', '--new_data_flag', 'False', '--monitoring_flag', 'True', '--latest_data_ndp_flag', '$AML_PARAMETER_latest_data_ndp_flag', '--ws_details', '/subscriptions/bdea9b80-e147-4dd1-9e22-afd1228b6d1a/resourceGroups/casa-churn-analysis-ey-demo/providers/Microsoft.MachineLearningServices/workspaces/AzureConnection'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'master-cpu-cluster-mtp', 'dataReferences': {}, 'data': {}, 'outputData': {'transaction_data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'bidv_blob_datastore', 'relativePath': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/transaction_data'}, 'uri': None, 'type': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': 'master_transaction_data_mtp', 'description': 'transaction_data for PipelineParameter_Name:observation_year_Default:2022_PipelineParameter_Name:observation_month_number_Default:10', 'tags': {'pipeline type': 'Model Training Pipeline (MTP)', 'model_version': '-1', 'pipeline_name': 'master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00', 'username': 'master', 'output_data_intermediate_path': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00', 'new_data_flag': 'False', 'schedule_pipeline_flag': 'False', 'latest_data_ndp_flag': 'False', 'data_path_to_monitor': 'InputData/InputRawFiles/transaction_data/', 'sample_flag': 'False', 'sample_size': '10000', 'reuse_sample_flag': 'False', 'observation_month_number': 'PipelineParameter_Name:observation_month_number_Default:10', 'observation_year': 'PipelineParameter_Name:observation_year_Default:2022', 'historical_months': 'PipelineParameter_Name:historical_months_Default:6', 'allow_reuse_step': 'True', 'regenerate_outputs': 'False', 'amlcompute_cluster_name': 'master-cpu-cluster-mtp', 'num_min_nodes': '0', 'num_max_nodes': '10', 'test_size': '0.1', 'outliers_method': 'iForest', 'outliers_threshold': '0.05', 'multiCollinearity_method': 'pearson', 'multiCollinearity_threshold': '0.95', 'featureSelection_method': 'lightGBM', 'featureSelection_threshold': '0', 'featureSelection_percentage': '1', 'normalization_method': 'skip', 'model_name': 'master_ChurnPrediction_Model', 'primary_metric': 'norm_macro_recall', 'user_selected_metric': '', 'iteration_timeout_minutes': '10', 'iterations': '40', 'experiment_timeout_hours': '3', 'max_concurrent_iterations': '10', 'enable_early_stopping': 'True', 'n_cross_validations': '5', 'validation_size': '0.1'}, 'properties': {'azureml.pipelineRunId': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipelineRun.moduleNodeId': 'ba45bd62', 'azureml.pipelineRun.outputPortName': 'transaction_data'}, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}, 'demographic_data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'bidv_blob_datastore', 'relativePath': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/demographic_data'}, 'uri': None, 'type': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': 'master_demographic_data_mtp', 'description': None, 'tags': {'pipeline type': 'Model Training Pipeline (MTP)', 'model_version': '-1', 'pipeline_name': 'master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00', 'username': 'master', 'output_data_intermediate_path': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00', 'new_data_flag': 'False', 'schedule_pipeline_flag': 'False', 'latest_data_ndp_flag': 'False', 'data_path_to_monitor': 'InputData/InputRawFiles/transaction_data/', 'sample_flag': 'False', 'sample_size': '10000', 'reuse_sample_flag': 'False', 'observation_month_number': 'PipelineParameter_Name:observation_month_number_Default:10', 'observation_year': 'PipelineParameter_Name:observation_year_Default:2022', 'historical_months': 'PipelineParameter_Name:historical_months_Default:6', 'allow_reuse_step': 'True', 'regenerate_outputs': 'False', 'amlcompute_cluster_name': 'master-cpu-cluster-mtp', 'num_min_nodes': '0', 'num_max_nodes': '10', 'test_size': '0.1', 'outliers_method': 'iForest', 'outliers_threshold': '0.05', 'multiCollinearity_method': 'pearson', 'multiCollinearity_threshold': '0.95', 'featureSelection_method': 'lightGBM', 'featureSelection_threshold': '0', 'featureSelection_percentage': '1', 'normalization_method': 'skip', 'model_name': 'master_ChurnPrediction_Model', 'primary_metric': 'norm_macro_recall', 'user_selected_metric': '', 'iteration_timeout_minutes': '10', 'iterations': '40', 'experiment_timeout_hours': '3', 'max_concurrent_iterations': '10', 'enable_early_stopping': 'True', 'n_cross_validations': '5', 'validation_size': '0.1'}, 'properties': {'azureml.pipelineRunId': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipelineRun.moduleNodeId': 'ba45bd62', 'azureml.pipelineRun.outputPortName': 'demographic_data'}, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}, 'account_data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'bidv_blob_datastore', 'relativePath': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/account_data'}, 'uri': None, 'type': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': 'master_account_data_mtp', 'description': None, 'tags': {'pipeline type': 'Model Training Pipeline (MTP)', 'model_version': '-1', 'pipeline_name': 'master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00', 'username': 'master', 'output_data_intermediate_path': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00', 'new_data_flag': 'False', 'schedule_pipeline_flag': 'False', 'latest_data_ndp_flag': 'False', 'data_path_to_monitor': 'InputData/InputRawFiles/transaction_data/', 'sample_flag': 'False', 'sample_size': '10000', 'reuse_sample_flag': 'False', 'observation_month_number': 'PipelineParameter_Name:observation_month_number_Default:10', 'observation_year': 'PipelineParameter_Name:observation_year_Default:2022', 'historical_months': 'PipelineParameter_Name:historical_months_Default:6', 'allow_reuse_step': 'True', 'regenerate_outputs': 'False', 'amlcompute_cluster_name': 'master-cpu-cluster-mtp', 'num_min_nodes': '0', 'num_max_nodes': '10', 'test_size': '0.1', 'outliers_method': 'iForest', 'outliers_threshold': '0.05', 'multiCollinearity_method': 'pearson', 'multiCollinearity_threshold': '0.95', 'featureSelection_method': 'lightGBM', 'featureSelection_threshold': '0', 'featureSelection_percentage': '1', 'normalization_method': 'skip', 'model_name': 'master_ChurnPrediction_Model', 'primary_metric': 'norm_macro_recall', 'user_selected_metric': '', 'iteration_timeout_minutes': '10', 'iterations': '40', 'experiment_timeout_hours': '3', 'max_concurrent_iterations': '10', 'enable_early_stopping': 'True', 'n_cross_validations': '5', 'validation_size': '0.1'}, 'properties': {'azureml.pipelineRunId': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipelineRun.moduleNodeId': 'ba45bd62', 'azureml.pipelineRun.outputPortName': 'account_data'}, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'default-environment', 'version': 'Autosave_2023-08-31T05:54:34Z_a2bfc594', 'assetId': 'azureml://locations/southeastasia/workspaces/bbf7f0e7-a482-4f81-8d49-aa10836bd1dc/environments/default-environment/versions/Autosave_2023-08-31T05:54:34Z_a2bfc594', 'autoRebuild': True, 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'name': 'project_environment', 'dependencies': ['python=3.8.13', {'pip': ['azureml-sdk[automl]~=1.51.0', 'azureml-fsspec', 'pyarrow>=0.16.0', 'ipywidgets', 'xgboost', 'joblib']}, 'pandas', 'scikit-learn'], 'channels': ['anaconda', 'conda-forge']}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20230509.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {'AML_PARAMETER_observation_year': '2022', 'AML_PARAMETER_observation_month_number': '10', 'AML_PARAMETER_historical_months': '6', 'AML_PARAMETER_latest_data_ndp_flag': 'False'}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/dataprep/0/rslex.log.2023-09-20-07': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/logs/azureml/dataprep/0/rslex.log.2023-09-20-07?sv=2019-07-07&sr=b&sig=skygQoMIt052i%2BuzRlOTK%2BMzfsJWbRRbrCZSALrAS90%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A05Z&se=2023-09-20T16%3A00%3A05Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=IAVtbJ7cFiRqP1ZLRHfHXt7ArUnAwUl1LkHvoQkNQLk%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A05Z&se=2023-09-20T16%3A00%3A05Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=D2WsjQW9DfIgT%2Bgn9YoVm0WUxMDkLGIJ%2B5ICj0UlsWQ%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A05Z&se=2023-09-20T16%3A00%3A05Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=TxFkcWIVBUjBQhzpxeXuVZiusmmk7gp9LyUUl3aqqNI%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A05Z&se=2023-09-20T16%3A00%3A05Z&sp=r', 'user_logs/std_log.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/user_logs/std_log.txt?sv=2019-07-07&sr=b&sig=nQWgtqqnT2unYOxjFXd5gY6kMMFoJniui6uhOeDQA%2B8%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A48Z&se=2023-09-20T16%3A00%3A48Z&sp=r', 'system_logs/cs_capability/cs-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/system_logs/cs_capability/cs-capability.log?sv=2019-07-07&sr=b&sig=r4ZxOdmpfC8OiJft5Bh%2BP3zUgsPmx8sW9X%2FCIIzmPs8%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A48Z&se=2023-09-20T16%3A00%3A48Z&sp=r', 'system_logs/data_capability/data-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/system_logs/data_capability/data-capability.log?sv=2019-07-07&sr=b&sig=HsNTHAxMShbgzjZddOJEiW24%2FCgsVvk2NbV1M1G3t2g%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A48Z&se=2023-09-20T16%3A00%3A48Z&sp=r', 'system_logs/data_capability/rslex.log.2023-09-20-07': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/system_logs/data_capability/rslex.log.2023-09-20-07?sv=2019-07-07&sr=b&sig=pb5A37YFP9cNhc9XVlf5gm78kPxd0a5mQH70oVlZB7c%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A48Z&se=2023-09-20T16%3A00%3A48Z&sp=r', 'system_logs/data_capability/rslex.log.2023-09-20-08': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/system_logs/data_capability/rslex.log.2023-09-20-08?sv=2019-07-07&sr=b&sig=Up3osrk0ftMCyr%2FkQ3igAbCKCOFKUb%2FzVq0PYI1k8kE%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A48Z&se=2023-09-20T16%3A00%3A48Z&sp=r', 'system_logs/hosttools_capability/hosttools-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/system_logs/hosttools_capability/hosttools-capability.log?sv=2019-07-07&sr=b&sig=hPOsjVS49T%2BqxtPr2mXZcy1CIIBWjTZ8dGcPpqqvpIc%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A48Z&se=2023-09-20T16%3A00%3A48Z&sp=r', 'system_logs/lifecycler/execution-wrapper.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/system_logs/lifecycler/execution-wrapper.log?sv=2019-07-07&sr=b&sig=yJhx61gBimSD9VqukehWGZDKlG3IBzay6GVOGgJJYL8%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A48Z&se=2023-09-20T16%3A00%3A48Z&sp=r', 'system_logs/lifecycler/lifecycler.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/system_logs/lifecycler/lifecycler.log?sv=2019-07-07&sr=b&sig=orl9z6RkQZrViVOFIyQukTbxSjcJwoFKZJvyNMWG9ik%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A48Z&se=2023-09-20T16%3A00%3A48Z&sp=r', 'system_logs/metrics_capability/metrics-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/system_logs/metrics_capability/metrics-capability.log?sv=2019-07-07&sr=b&sig=IGbNpK7gpE7%2BBYXqMw3NXyeuWcEvxSubHDZiaQgdDiE%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A48Z&se=2023-09-20T16%3A00%3A48Z&sp=r', 'system_logs/snapshot_capability/snapshot-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.1630a665-6cc8-482e-ab69-2f33e1a2e993/system_logs/snapshot_capability/snapshot-capability.log?sv=2019-07-07&sr=b&sig=Z8J%2B%2Fg7Ree%2FmngLRVAIr8I7Q2OofoDvjBLF1%2Bnhv9V8%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A50%3A48Z&se=2023-09-20T16%3A00%3A48Z&sp=r'}, 'submittedBy': 'Khanh Van Le'}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "StepRunId: 9db37187-f6f4-4d67-8549-1d2722c1f0b4\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/9db37187-f6f4-4d67-8549-1d2722c1f0b4?wsid=/subscriptions/bdea9b80-e147-4dd1-9e22-afd1228b6d1a/resourcegroups/casa-churn-analysis-ey-demo/workspaces/AzureConnection&tid=55587992-a704-412d-a52c-ce2a96ed3cf4\n",
            "StepRun( Transform_Account_Data ) Status: NotStarted\n",
            "StepRun( Transform_Account_Data ) Status: Running\n",
            "\n",
            "StepRun(Transform_Account_Data) Execution Summary\n",
            "==================================================\n",
            "StepRun( Transform_Account_Data ) Status: Finished\n",
            "{'runId': '9db37187-f6f4-4d67-8549-1d2722c1f0b4', 'target': 'master-cpu-cluster-mtp', 'status': 'Completed', 'startTimeUtc': '2023-09-20T08:02:50.810589Z', 'endTimeUtc': '2023-09-20T08:05:01.73732Z', 'services': {}, 'properties': {'ContentSnapshotId': '9f62dd6d-96e9-4984-869e-1459dc648a51', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'f242db27-3d7d-43eb-b328-9831c01e16af', 'azureml.moduleName': 'Transform_Account_Data', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': 'f9c3f177', 'azureml.pipelinerunid': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipeline': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.rootpipelinerunid': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlctrain', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '80c6a065-4177-4486-9c10-9894adf03d01'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'raw_data_account', 'mechanism': 'Direct'}}, {'dataset': {'id': 'cf160736-9318-4f03-8693-2ad6cb47292f'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'mapping_data_translation', 'mechanism': 'Direct'}}], 'outputDatasets': [{'identifier': {'savedId': '6c5d26d9-c6bb-4175-9a29-ec3b0a5df51b', 'registeredId': '1e876d6d-ed0d-4df3-97d9-70ab3dbf14b3', 'registeredVersion': '3'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'transformed_account_data'}, 'dataset': {\n",
            "  \"source\": [\n",
            "    \"('bidv_blob_datastore', 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/transformed_account_data')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"6c5d26d9-c6bb-4175-9a29-ec3b0a5df51b\",\n",
            "    \"name\": \"master_transformed_account_data_mtp\",\n",
            "    \"version\": 3,\n",
            "    \"tags\": {\n",
            "      \"pipeline type\": \"Model Training Pipeline (MTP)\",\n",
            "      \"model_version\": \"-1\",\n",
            "      \"pipeline_name\": \"master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00\",\n",
            "      \"username\": \"master\",\n",
            "      \"output_data_intermediate_path\": \"Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00\",\n",
            "      \"new_data_flag\": \"False\",\n",
            "      \"schedule_pipeline_flag\": \"False\",\n",
            "      \"latest_data_ndp_flag\": \"False\",\n",
            "      \"data_path_to_monitor\": \"InputData/InputRawFiles/transaction_data/\",\n",
            "      \"sample_flag\": \"False\",\n",
            "      \"sample_size\": \"10000\",\n",
            "      \"reuse_sample_flag\": \"False\",\n",
            "      \"observation_month_number\": \"PipelineParameter_Name:observation_month_number_Default:10\",\n",
            "      \"observation_year\": \"PipelineParameter_Name:observation_year_Default:2022\",\n",
            "      \"historical_months\": \"PipelineParameter_Name:historical_months_Default:6\",\n",
            "      \"allow_reuse_step\": \"True\",\n",
            "      \"regenerate_outputs\": \"False\",\n",
            "      \"amlcompute_cluster_name\": \"master-cpu-cluster-mtp\",\n",
            "      \"num_min_nodes\": \"0\",\n",
            "      \"num_max_nodes\": \"10\",\n",
            "      \"test_size\": \"0.1\",\n",
            "      \"outliers_method\": \"iForest\",\n",
            "      \"outliers_threshold\": \"0.05\",\n",
            "      \"multiCollinearity_method\": \"pearson\",\n",
            "      \"multiCollinearity_threshold\": \"0.95\",\n",
            "      \"featureSelection_method\": \"lightGBM\",\n",
            "      \"featureSelection_threshold\": \"0\",\n",
            "      \"featureSelection_percentage\": \"1\",\n",
            "      \"normalization_method\": \"skip\",\n",
            "      \"model_name\": \"master_ChurnPrediction_Model\",\n",
            "      \"primary_metric\": \"norm_macro_recall\",\n",
            "      \"user_selected_metric\": \"\",\n",
            "      \"iteration_timeout_minutes\": \"10\",\n",
            "      \"iterations\": \"40\",\n",
            "      \"experiment_timeout_hours\": \"3\",\n",
            "      \"max_concurrent_iterations\": \"10\",\n",
            "      \"enable_early_stopping\": \"True\",\n",
            "      \"n_cross_validations\": \"5\",\n",
            "      \"validation_size\": \"0.1\"\n",
            "    },\n",
            "    \"workspace\": \"Workspace.create(name='AzureConnection', subscription_id='bdea9b80-e147-4dd1-9e22-afd1228b6d1a', resource_group='casa-churn-analysis-ey-demo')\"\n",
            "  }\n",
            "}}], 'runDefinition': {'script': 'transform_account.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--output_data_path', 'DatasetOutputConfig:transformed_account_data'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'master-cpu-cluster-mtp', 'dataReferences': {}, 'data': {'raw_data_account': {'dataLocation': {'dataset': {'id': '80c6a065-4177-4486-9c10-9894adf03d01', 'name': None, 'version': None}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'raw_data_account', 'pathOnCompute': None, 'overwrite': False, 'options': None}, 'mapping_data_translation': {'dataLocation': {'dataset': {'id': 'cf160736-9318-4f03-8693-2ad6cb47292f', 'name': None, 'version': '1'}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'mapping_data_translation', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {'transformed_account_data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'bidv_blob_datastore', 'relativePath': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/transformed_account_data'}, 'uri': None, 'type': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': 'master_transformed_account_data_mtp', 'description': None, 'tags': {'pipeline type': 'Model Training Pipeline (MTP)', 'model_version': '-1', 'pipeline_name': 'master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00', 'username': 'master', 'output_data_intermediate_path': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00', 'new_data_flag': 'False', 'schedule_pipeline_flag': 'False', 'latest_data_ndp_flag': 'False', 'data_path_to_monitor': 'InputData/InputRawFiles/transaction_data/', 'sample_flag': 'False', 'sample_size': '10000', 'reuse_sample_flag': 'False', 'observation_month_number': 'PipelineParameter_Name:observation_month_number_Default:10', 'observation_year': 'PipelineParameter_Name:observation_year_Default:2022', 'historical_months': 'PipelineParameter_Name:historical_months_Default:6', 'allow_reuse_step': 'True', 'regenerate_outputs': 'False', 'amlcompute_cluster_name': 'master-cpu-cluster-mtp', 'num_min_nodes': '0', 'num_max_nodes': '10', 'test_size': '0.1', 'outliers_method': 'iForest', 'outliers_threshold': '0.05', 'multiCollinearity_method': 'pearson', 'multiCollinearity_threshold': '0.95', 'featureSelection_method': 'lightGBM', 'featureSelection_threshold': '0', 'featureSelection_percentage': '1', 'normalization_method': 'skip', 'model_name': 'master_ChurnPrediction_Model', 'primary_metric': 'norm_macro_recall', 'user_selected_metric': '', 'iteration_timeout_minutes': '10', 'iterations': '40', 'experiment_timeout_hours': '3', 'max_concurrent_iterations': '10', 'enable_early_stopping': 'True', 'n_cross_validations': '5', 'validation_size': '0.1'}, 'properties': {'azureml.pipelineRunId': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipelineRun.moduleNodeId': 'f9c3f177', 'azureml.pipelineRun.outputPortName': 'transformed_account_data'}, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'default-environment', 'version': 'Autosave_2023-08-31T05:54:34Z_a2bfc594', 'assetId': 'azureml://locations/southeastasia/workspaces/bbf7f0e7-a482-4f81-8d49-aa10836bd1dc/environments/default-environment/versions/Autosave_2023-08-31T05:54:34Z_a2bfc594', 'autoRebuild': True, 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'name': 'project_environment', 'dependencies': ['python=3.8.13', {'pip': ['azureml-sdk[automl]~=1.51.0', 'azureml-fsspec', 'pyarrow>=0.16.0', 'ipywidgets', 'xgboost', 'joblib']}, 'pandas', 'scikit-learn'], 'channels': ['anaconda', 'conda-forge']}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20230509.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/dataprep/0/rslex.log.2023-09-20-08': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/logs/azureml/dataprep/0/rslex.log.2023-09-20-08?sv=2019-07-07&sr=b&sig=rCT7cwC5%2FdADlREIkpjc7i4IP7VPYBvdXuv%2FDGGCVXc%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A53%3A22Z&se=2023-09-20T16%3A03%3A22Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=RAK%2F5H%2FUroswh1hGoZTnJozMeoE5GFtmV4DvY1ko3R4%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A53%3A22Z&se=2023-09-20T16%3A03%3A22Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=B8T7pR8WADRkec%2Bke%2BtOg0xAPNNTAXDAONlmjTHqOK0%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A53%3A22Z&se=2023-09-20T16%3A03%3A22Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=i2gJzEJyI5L%2FNpsOMpbNlUGoL1QNRXhyoDhVmMhGVCw%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A53%3A22Z&se=2023-09-20T16%3A03%3A22Z&sp=r', 'user_logs/std_log.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/user_logs/std_log.txt?sv=2019-07-07&sr=b&sig=vHiijnWFKMvDxdbqm2fJzefmHUkIiS57CTu4uqzau%2F4%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/cs_capability/cs-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/system_logs/cs_capability/cs-capability.log?sv=2019-07-07&sr=b&sig=FjfY1VVvwe8YO8I9VXNvU37I6PgEs4tFbLK%2FOS2DJcM%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/data_capability/data-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/system_logs/data_capability/data-capability.log?sv=2019-07-07&sr=b&sig=1OJVU02DQPRWyk%2BRPTVeFaouRkAU1Irv18m9htzFW5Q%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/data_capability/rslex.log.2023-09-20-08': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/system_logs/data_capability/rslex.log.2023-09-20-08?sv=2019-07-07&sr=b&sig=JCWe7ma6OKyiBjDzlvMYGcHnxPRvyraAWGuKqz1mHmE%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/hosttools_capability/hosttools-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/system_logs/hosttools_capability/hosttools-capability.log?sv=2019-07-07&sr=b&sig=ljE3bFa09wyBml2iHPG6de3b5U%2Fld07LkdzngzGFwSc%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/lifecycler/execution-wrapper.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/system_logs/lifecycler/execution-wrapper.log?sv=2019-07-07&sr=b&sig=68PWRmBwhAY5GYSdZiCyU0AZGR7H%2BAhskMNQQ0pFyHU%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/lifecycler/lifecycler.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/system_logs/lifecycler/lifecycler.log?sv=2019-07-07&sr=b&sig=mr9sq1P%2FJDPq3VsTWyickmqouLahgTUbghVnAtoeoag%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/metrics_capability/metrics-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/system_logs/metrics_capability/metrics-capability.log?sv=2019-07-07&sr=b&sig=ydgcUztgGpjA44EE9%2B47UnNo7UXn8S9LOHOrQGiXPUk%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/snapshot_capability/snapshot-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.9db37187-f6f4-4d67-8549-1d2722c1f0b4/system_logs/snapshot_capability/snapshot-capability.log?sv=2019-07-07&sr=b&sig=wsSXQIw1vCGM7%2Fl5SUGsa7i4jSOAK%2BS3BQSRAuRv5mw%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r'}, 'submittedBy': 'Khanh Van Le'}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "StepRunId: 17d43bdd-730b-4db1-9582-369beaedac30\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/17d43bdd-730b-4db1-9582-369beaedac30?wsid=/subscriptions/bdea9b80-e147-4dd1-9e22-afd1228b6d1a/resourcegroups/casa-churn-analysis-ey-demo/workspaces/AzureConnection&tid=55587992-a704-412d-a52c-ce2a96ed3cf4\n",
            "\n",
            "StepRun(Transform_Demographics_Data) Execution Summary\n",
            "=======================================================\n",
            "StepRun( Transform_Demographics_Data ) Status: Finished\n",
            "{'runId': '17d43bdd-730b-4db1-9582-369beaedac30', 'target': 'master-cpu-cluster-mtp', 'status': 'Completed', 'startTimeUtc': '2023-09-20T08:01:01.979701Z', 'endTimeUtc': '2023-09-20T08:02:46.416648Z', 'services': {}, 'properties': {'ContentSnapshotId': '04402b47-ee46-4927-982f-53b270435263', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'ef00461e-0ca2-4720-8d37-f135922ccbe0', 'azureml.moduleName': 'Transform_Demographics_Data', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '5837c670', 'azureml.pipelinerunid': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipeline': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.rootpipelinerunid': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlctrain', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '274b75b8-46e7-485a-b4c6-66deb76b065f'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'raw_data_demographics', 'mechanism': 'Direct'}}, {'dataset': {'id': 'ba736ae8-f28b-4531-bd72-0b658c9634bd'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'mapping_data_region', 'mechanism': 'Direct'}}, {'dataset': {'id': '07e0bb04-0392-455d-917b-0911d945157f'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'mapping_data_occupation', 'mechanism': 'Direct'}}, {'dataset': {'id': 'cf160736-9318-4f03-8693-2ad6cb47292f'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'mapping_data_translation', 'mechanism': 'Direct'}}], 'outputDatasets': [{'identifier': {'savedId': 'b6cd2c77-488f-4894-8e2d-b1aa01773a40', 'registeredId': '0baa8a7d-7647-4979-877c-4febd442cb06', 'registeredVersion': '3'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'transformed_demographics_data'}, 'dataset': {\n",
            "  \"source\": [\n",
            "    \"('bidv_blob_datastore', 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/transformed_demographics_data')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"b6cd2c77-488f-4894-8e2d-b1aa01773a40\",\n",
            "    \"name\": \"master_transformed_demographics_data_mtp\",\n",
            "    \"version\": 3,\n",
            "    \"tags\": {\n",
            "      \"pipeline type\": \"Model Training Pipeline (MTP)\",\n",
            "      \"model_version\": \"-1\",\n",
            "      \"pipeline_name\": \"master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00\",\n",
            "      \"username\": \"master\",\n",
            "      \"output_data_intermediate_path\": \"Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00\",\n",
            "      \"new_data_flag\": \"False\",\n",
            "      \"schedule_pipeline_flag\": \"False\",\n",
            "      \"latest_data_ndp_flag\": \"False\",\n",
            "      \"data_path_to_monitor\": \"InputData/InputRawFiles/transaction_data/\",\n",
            "      \"sample_flag\": \"False\",\n",
            "      \"sample_size\": \"10000\",\n",
            "      \"reuse_sample_flag\": \"False\",\n",
            "      \"observation_month_number\": \"PipelineParameter_Name:observation_month_number_Default:10\",\n",
            "      \"observation_year\": \"PipelineParameter_Name:observation_year_Default:2022\",\n",
            "      \"historical_months\": \"PipelineParameter_Name:historical_months_Default:6\",\n",
            "      \"allow_reuse_step\": \"True\",\n",
            "      \"regenerate_outputs\": \"False\",\n",
            "      \"amlcompute_cluster_name\": \"master-cpu-cluster-mtp\",\n",
            "      \"num_min_nodes\": \"0\",\n",
            "      \"num_max_nodes\": \"10\",\n",
            "      \"test_size\": \"0.1\",\n",
            "      \"outliers_method\": \"iForest\",\n",
            "      \"outliers_threshold\": \"0.05\",\n",
            "      \"multiCollinearity_method\": \"pearson\",\n",
            "      \"multiCollinearity_threshold\": \"0.95\",\n",
            "      \"featureSelection_method\": \"lightGBM\",\n",
            "      \"featureSelection_threshold\": \"0\",\n",
            "      \"featureSelection_percentage\": \"1\",\n",
            "      \"normalization_method\": \"skip\",\n",
            "      \"model_name\": \"master_ChurnPrediction_Model\",\n",
            "      \"primary_metric\": \"norm_macro_recall\",\n",
            "      \"user_selected_metric\": \"\",\n",
            "      \"iteration_timeout_minutes\": \"10\",\n",
            "      \"iterations\": \"40\",\n",
            "      \"experiment_timeout_hours\": \"3\",\n",
            "      \"max_concurrent_iterations\": \"10\",\n",
            "      \"enable_early_stopping\": \"True\",\n",
            "      \"n_cross_validations\": \"5\",\n",
            "      \"validation_size\": \"0.1\"\n",
            "    },\n",
            "    \"workspace\": \"Workspace.create(name='AzureConnection', subscription_id='bdea9b80-e147-4dd1-9e22-afd1228b6d1a', resource_group='casa-churn-analysis-ey-demo')\"\n",
            "  }\n",
            "}}], 'runDefinition': {'script': 'transform_demographics.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--output_data_path', 'DatasetOutputConfig:transformed_demographics_data'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'master-cpu-cluster-mtp', 'dataReferences': {}, 'data': {'raw_data_demographics': {'dataLocation': {'dataset': {'id': '274b75b8-46e7-485a-b4c6-66deb76b065f', 'name': None, 'version': None}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'raw_data_demographics', 'pathOnCompute': None, 'overwrite': False, 'options': None}, 'mapping_data_region': {'dataLocation': {'dataset': {'id': 'ba736ae8-f28b-4531-bd72-0b658c9634bd', 'name': None, 'version': '1'}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'mapping_data_region', 'pathOnCompute': None, 'overwrite': False, 'options': None}, 'mapping_data_occupation': {'dataLocation': {'dataset': {'id': '07e0bb04-0392-455d-917b-0911d945157f', 'name': None, 'version': '1'}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'mapping_data_occupation', 'pathOnCompute': None, 'overwrite': False, 'options': None}, 'mapping_data_translation': {'dataLocation': {'dataset': {'id': 'cf160736-9318-4f03-8693-2ad6cb47292f', 'name': None, 'version': '1'}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'mapping_data_translation', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {'transformed_demographics_data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'bidv_blob_datastore', 'relativePath': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/transformed_demographics_data'}, 'uri': None, 'type': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': 'master_transformed_demographics_data_mtp', 'description': None, 'tags': {'pipeline type': 'Model Training Pipeline (MTP)', 'model_version': '-1', 'pipeline_name': 'master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00', 'username': 'master', 'output_data_intermediate_path': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00', 'new_data_flag': 'False', 'schedule_pipeline_flag': 'False', 'latest_data_ndp_flag': 'False', 'data_path_to_monitor': 'InputData/InputRawFiles/transaction_data/', 'sample_flag': 'False', 'sample_size': '10000', 'reuse_sample_flag': 'False', 'observation_month_number': 'PipelineParameter_Name:observation_month_number_Default:10', 'observation_year': 'PipelineParameter_Name:observation_year_Default:2022', 'historical_months': 'PipelineParameter_Name:historical_months_Default:6', 'allow_reuse_step': 'True', 'regenerate_outputs': 'False', 'amlcompute_cluster_name': 'master-cpu-cluster-mtp', 'num_min_nodes': '0', 'num_max_nodes': '10', 'test_size': '0.1', 'outliers_method': 'iForest', 'outliers_threshold': '0.05', 'multiCollinearity_method': 'pearson', 'multiCollinearity_threshold': '0.95', 'featureSelection_method': 'lightGBM', 'featureSelection_threshold': '0', 'featureSelection_percentage': '1', 'normalization_method': 'skip', 'model_name': 'master_ChurnPrediction_Model', 'primary_metric': 'norm_macro_recall', 'user_selected_metric': '', 'iteration_timeout_minutes': '10', 'iterations': '40', 'experiment_timeout_hours': '3', 'max_concurrent_iterations': '10', 'enable_early_stopping': 'True', 'n_cross_validations': '5', 'validation_size': '0.1'}, 'properties': {'azureml.pipelineRunId': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipelineRun.moduleNodeId': '5837c670', 'azureml.pipelineRun.outputPortName': 'transformed_demographics_data'}, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'default-environment', 'version': 'Autosave_2023-08-31T05:54:34Z_a2bfc594', 'assetId': 'azureml://locations/southeastasia/workspaces/bbf7f0e7-a482-4f81-8d49-aa10836bd1dc/environments/default-environment/versions/Autosave_2023-08-31T05:54:34Z_a2bfc594', 'autoRebuild': True, 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'name': 'project_environment', 'dependencies': ['python=3.8.13', {'pip': ['azureml-sdk[automl]~=1.51.0', 'azureml-fsspec', 'pyarrow>=0.16.0', 'ipywidgets', 'xgboost', 'joblib']}, 'pandas', 'scikit-learn'], 'channels': ['anaconda', 'conda-forge']}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20230509.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/dataprep/0/rslex.log.2023-09-20-08': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/logs/azureml/dataprep/0/rslex.log.2023-09-20-08?sv=2019-07-07&sr=b&sig=L0Gta%2BEvWSt8J6jCMZuIztYBWlm2qlkd6IFw7Krj4uo%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=Cv2VkYGd79FAsoOohURjSDazBRF%2FFIgc%2FBisAufYnWg%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=SVtdXGv5R7MUQz%2BoLm1EauUdj4dmGfE%2FQ%2BMyrk1shyU%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=RuxcnKYLwgYJ7NNQumhIGfEO%2Fb1SCS3%2BToj8vxy5mYA%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'user_logs/std_log.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/user_logs/std_log.txt?sv=2019-07-07&sr=b&sig=erUNY7cFj6z0LFMshRFPN1UD8%2FMuXxKK95WSQ%2BuzyJQ%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/cs_capability/cs-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/system_logs/cs_capability/cs-capability.log?sv=2019-07-07&sr=b&sig=TTK1kziU1MgXfhdb5CuvpSTdJ8ALWiaZ5P228KSL6EM%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/data_capability/data-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/system_logs/data_capability/data-capability.log?sv=2019-07-07&sr=b&sig=A%2BYd6f2C9%2FmP1CYUIa7YJzI%2BNMP7TVBdnYpASrGrQew%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/data_capability/rslex.log.2023-09-20-08': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/system_logs/data_capability/rslex.log.2023-09-20-08?sv=2019-07-07&sr=b&sig=l5wPE9okXLiEJksl4%2BRG0fhQDXjbS2kGtzM%2BY8fbdZ4%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/hosttools_capability/hosttools-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/system_logs/hosttools_capability/hosttools-capability.log?sv=2019-07-07&sr=b&sig=CTc3YavPCDaUMV6xpxSdVmKyS%2FefMAZWWfJOQpOa4lw%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/lifecycler/execution-wrapper.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/system_logs/lifecycler/execution-wrapper.log?sv=2019-07-07&sr=b&sig=IrW%2B7zdRc5cVf%2FpDsBlKLRdyMAo7uo717QIEJb2OAzg%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/lifecycler/lifecycler.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/system_logs/lifecycler/lifecycler.log?sv=2019-07-07&sr=b&sig=NqX7uMDGLGgly6IqSQrCkemj0ibAUtud%2B5Lra8iM5Pc%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/metrics_capability/metrics-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/system_logs/metrics_capability/metrics-capability.log?sv=2019-07-07&sr=b&sig=VmGTrWhjTqhMWtOMA9i3o27sCYRz7v3y7al264qSaiA%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r', 'system_logs/snapshot_capability/snapshot-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.17d43bdd-730b-4db1-9582-369beaedac30/system_logs/snapshot_capability/snapshot-capability.log?sv=2019-07-07&sr=b&sig=asLr3GdzjdMnbXiwFTHtm6gLkdZhp08OUxWcYAXM4ME%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T07%3A55%3A04Z&se=2023-09-20T16%3A05%3A04Z&sp=r'}, 'submittedBy': 'Khanh Van Le'}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "StepRunId: a952eb20-4134-49fe-9016-3eb604efb6ff\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/a952eb20-4134-49fe-9016-3eb604efb6ff?wsid=/subscriptions/bdea9b80-e147-4dd1-9e22-afd1228b6d1a/resourcegroups/casa-churn-analysis-ey-demo/workspaces/AzureConnection&tid=55587992-a704-412d-a52c-ce2a96ed3cf4\n",
            "StepRun( Transform_Transaction_Data ) Status: Running\n",
            "\n",
            "StepRun(Transform_Transaction_Data) Execution Summary\n",
            "======================================================\n",
            "StepRun( Transform_Transaction_Data ) Status: Finished\n",
            "{'runId': 'a952eb20-4134-49fe-9016-3eb604efb6ff', 'target': 'master-cpu-cluster-mtp', 'status': 'Completed', 'startTimeUtc': '2023-09-20T08:03:14.1871Z', 'endTimeUtc': '2023-09-20T08:31:35.426755Z', 'services': {}, 'properties': {'ContentSnapshotId': '50da393c-7b8c-4f63-9a50-9cdfd3631b85', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'e5082507-8c09-47ff-b969-60e69f83e5cd', 'azureml.moduleName': 'Transform_Transaction_Data', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '47be9d02', 'azureml.pipelinerunid': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipeline': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.rootpipelinerunid': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlctrain', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '6cc1c16d-06cf-458c-b55f-04d9a0da0a82'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'raw_data_transaction', 'mechanism': 'Direct'}}, {'dataset': {'id': 'cf160736-9318-4f03-8693-2ad6cb47292f'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'mapping_data_translation', 'mechanism': 'Direct'}}, {'dataset': {'id': 'fb70c994-fee1-4b72-a262-90ba6b40505c'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'transformed_demographics_data', 'mechanism': 'Direct'}}], 'outputDatasets': [{'identifier': {'savedId': '943209fe-9740-4507-9b5f-34ca07c25f34', 'registeredId': '469eb33b-d1d4-49de-8cf4-11a5cdebb1d5', 'registeredVersion': '3'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'transformed_transaction_data'}, 'dataset': {\n",
            "  \"source\": [\n",
            "    \"('bidv_blob_datastore', 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/transformed_transaction_data')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"943209fe-9740-4507-9b5f-34ca07c25f34\",\n",
            "    \"name\": \"master_transformed_transaction_data_mtp\",\n",
            "    \"version\": 3,\n",
            "    \"tags\": {\n",
            "      \"pipeline type\": \"Model Training Pipeline (MTP)\",\n",
            "      \"model_version\": \"-1\",\n",
            "      \"pipeline_name\": \"master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00\",\n",
            "      \"username\": \"master\",\n",
            "      \"output_data_intermediate_path\": \"Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00\",\n",
            "      \"new_data_flag\": \"False\",\n",
            "      \"schedule_pipeline_flag\": \"False\",\n",
            "      \"latest_data_ndp_flag\": \"False\",\n",
            "      \"data_path_to_monitor\": \"InputData/InputRawFiles/transaction_data/\",\n",
            "      \"sample_flag\": \"False\",\n",
            "      \"sample_size\": \"10000\",\n",
            "      \"reuse_sample_flag\": \"False\",\n",
            "      \"observation_month_number\": \"PipelineParameter_Name:observation_month_number_Default:10\",\n",
            "      \"observation_year\": \"PipelineParameter_Name:observation_year_Default:2022\",\n",
            "      \"historical_months\": \"PipelineParameter_Name:historical_months_Default:6\",\n",
            "      \"allow_reuse_step\": \"True\",\n",
            "      \"regenerate_outputs\": \"False\",\n",
            "      \"amlcompute_cluster_name\": \"master-cpu-cluster-mtp\",\n",
            "      \"num_min_nodes\": \"0\",\n",
            "      \"num_max_nodes\": \"10\",\n",
            "      \"test_size\": \"0.1\",\n",
            "      \"outliers_method\": \"iForest\",\n",
            "      \"outliers_threshold\": \"0.05\",\n",
            "      \"multiCollinearity_method\": \"pearson\",\n",
            "      \"multiCollinearity_threshold\": \"0.95\",\n",
            "      \"featureSelection_method\": \"lightGBM\",\n",
            "      \"featureSelection_threshold\": \"0\",\n",
            "      \"featureSelection_percentage\": \"1\",\n",
            "      \"normalization_method\": \"skip\",\n",
            "      \"model_name\": \"master_ChurnPrediction_Model\",\n",
            "      \"primary_metric\": \"norm_macro_recall\",\n",
            "      \"user_selected_metric\": \"\",\n",
            "      \"iteration_timeout_minutes\": \"10\",\n",
            "      \"iterations\": \"40\",\n",
            "      \"experiment_timeout_hours\": \"3\",\n",
            "      \"max_concurrent_iterations\": \"10\",\n",
            "      \"enable_early_stopping\": \"True\",\n",
            "      \"n_cross_validations\": \"5\",\n",
            "      \"validation_size\": \"0.1\"\n",
            "    },\n",
            "    \"workspace\": \"Workspace.create(name='AzureConnection', subscription_id='bdea9b80-e147-4dd1-9e22-afd1228b6d1a', resource_group='casa-churn-analysis-ey-demo')\"\n",
            "  }\n",
            "}}], 'runDefinition': {'script': 'transform_transaction.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--output_data_path', 'DatasetOutputConfig:transformed_transaction_data', '--njobs', '19'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'master-cpu-cluster-mtp', 'dataReferences': {}, 'data': {'raw_data_transaction': {'dataLocation': {'dataset': {'id': '6cc1c16d-06cf-458c-b55f-04d9a0da0a82', 'name': None, 'version': None}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'raw_data_transaction', 'pathOnCompute': None, 'overwrite': False, 'options': None}, 'mapping_data_translation': {'dataLocation': {'dataset': {'id': 'cf160736-9318-4f03-8693-2ad6cb47292f', 'name': None, 'version': '1'}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'mapping_data_translation', 'pathOnCompute': None, 'overwrite': False, 'options': None}, 'transformed_demographics_data': {'dataLocation': {'dataset': {'id': 'fb70c994-fee1-4b72-a262-90ba6b40505c', 'name': None, 'version': None}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'transformed_demographics_data', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {'transformed_transaction_data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'bidv_blob_datastore', 'relativePath': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00/transformed_transaction_data'}, 'uri': None, 'type': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': 'master_transformed_transaction_data_mtp', 'description': None, 'tags': {'pipeline type': 'Model Training Pipeline (MTP)', 'model_version': '-1', 'pipeline_name': 'master_Pipeline-DataPrep-ModelTrainTest_20_09_2023_07_55_00', 'username': 'master', 'output_data_intermediate_path': 'Users/master/MTP/Outputs/OutputDataset/20_09_2023_07_55_00', 'new_data_flag': 'False', 'schedule_pipeline_flag': 'False', 'latest_data_ndp_flag': 'False', 'data_path_to_monitor': 'InputData/InputRawFiles/transaction_data/', 'sample_flag': 'False', 'sample_size': '10000', 'reuse_sample_flag': 'False', 'observation_month_number': 'PipelineParameter_Name:observation_month_number_Default:10', 'observation_year': 'PipelineParameter_Name:observation_year_Default:2022', 'historical_months': 'PipelineParameter_Name:historical_months_Default:6', 'allow_reuse_step': 'True', 'regenerate_outputs': 'False', 'amlcompute_cluster_name': 'master-cpu-cluster-mtp', 'num_min_nodes': '0', 'num_max_nodes': '10', 'test_size': '0.1', 'outliers_method': 'iForest', 'outliers_threshold': '0.05', 'multiCollinearity_method': 'pearson', 'multiCollinearity_threshold': '0.95', 'featureSelection_method': 'lightGBM', 'featureSelection_threshold': '0', 'featureSelection_percentage': '1', 'normalization_method': 'skip', 'model_name': 'master_ChurnPrediction_Model', 'primary_metric': 'norm_macro_recall', 'user_selected_metric': '', 'iteration_timeout_minutes': '10', 'iterations': '40', 'experiment_timeout_hours': '3', 'max_concurrent_iterations': '10', 'enable_early_stopping': 'True', 'n_cross_validations': '5', 'validation_size': '0.1'}, 'properties': {'azureml.pipelineRunId': 'd0d2ac3e-43a0-4cba-8d9b-6f6e77a38d3c', 'azureml.pipelineRun.moduleNodeId': '47be9d02', 'azureml.pipelineRun.outputPortName': 'transformed_transaction_data'}, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'default-environment', 'version': 'Autosave_2023-08-31T05:54:34Z_a2bfc594', 'assetId': 'azureml://locations/southeastasia/workspaces/bbf7f0e7-a482-4f81-8d49-aa10836bd1dc/environments/default-environment/versions/Autosave_2023-08-31T05:54:34Z_a2bfc594', 'autoRebuild': True, 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'name': 'project_environment', 'dependencies': ['python=3.8.13', {'pip': ['azureml-sdk[automl]~=1.51.0', 'azureml-fsspec', 'pyarrow>=0.16.0', 'ipywidgets', 'xgboost', 'joblib']}, 'pandas', 'scikit-learn'], 'channels': ['anaconda', 'conda-forge']}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20230509.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/dataprep/0/rslex.log.2023-09-20-08': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/logs/azureml/dataprep/0/rslex.log.2023-09-20-08?sv=2019-07-07&sr=b&sig=S3Kh4qmlrIXog1MIFPmJletEKJamyBcqfIAwSBC3v%2FE%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A09Z&se=2023-09-20T16%3A31%3A09Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=rtEyKPLFOSu2qOYjIGSXLpSYV1cvNDvpUaaHzgyJsN8%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A09Z&se=2023-09-20T16%3A31%3A09Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=oTMCsHqytxWC3i1xD7GqZXKOAvOVuHavvpeVgaCzPgk%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A09Z&se=2023-09-20T16%3A31%3A09Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=ey3oOJzzUNZLTpMZpA4xFYd%2BtezHCQ3gS4N46x6tWWk%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A09Z&se=2023-09-20T16%3A31%3A09Z&sp=r', 'user_logs/std_log.txt': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/user_logs/std_log.txt?sv=2019-07-07&sr=b&sig=Oe92bmRhoYBZCsF5uLuZa2AFR3xfZYyE7HyuXYXmZc8%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A38Z&se=2023-09-20T16%3A31%3A38Z&sp=r', 'system_logs/cs_capability/cs-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/system_logs/cs_capability/cs-capability.log?sv=2019-07-07&sr=b&sig=Nc73mS42s560w12zAVa%2FSFkuNzIPKXkxBlWdVAlYzQs%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A38Z&se=2023-09-20T16%3A31%3A38Z&sp=r', 'system_logs/data_capability/data-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/system_logs/data_capability/data-capability.log?sv=2019-07-07&sr=b&sig=DeyeXIhMRlKbZSvFXJKhHkeFhNCSHucJh5XKWifTZpE%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A38Z&se=2023-09-20T16%3A31%3A38Z&sp=r', 'system_logs/data_capability/rslex.log.2023-09-20-08': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/system_logs/data_capability/rslex.log.2023-09-20-08?sv=2019-07-07&sr=b&sig=DKl%2B7c22ZABbD49z%2B2tHpQwuITLfPFQPwP4LWZkQJ4s%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A38Z&se=2023-09-20T16%3A31%3A38Z&sp=r', 'system_logs/hosttools_capability/hosttools-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/system_logs/hosttools_capability/hosttools-capability.log?sv=2019-07-07&sr=b&sig=etbII27u4XlQmNhXChjC922dJdyKpC6vK3rPpzU1BUs%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A38Z&se=2023-09-20T16%3A31%3A38Z&sp=r', 'system_logs/lifecycler/execution-wrapper.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/system_logs/lifecycler/execution-wrapper.log?sv=2019-07-07&sr=b&sig=8hXKlgd6kV6s58dRB%2FqJTNsbj%2Fx9LvLnShHaweiLIFU%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A38Z&se=2023-09-20T16%3A31%3A38Z&sp=r', 'system_logs/lifecycler/lifecycler.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/system_logs/lifecycler/lifecycler.log?sv=2019-07-07&sr=b&sig=YeV2z8RC41pXuWTEWqJ%2BIbn375VYy239UBLJsiuZIT8%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A38Z&se=2023-09-20T16%3A31%3A38Z&sp=r', 'system_logs/metrics_capability/metrics-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/system_logs/metrics_capability/metrics-capability.log?sv=2019-07-07&sr=b&sig=86%2BEEal5LVZPSpDVWRCQxahOclhRfpV8RLZULIMnG34%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A38Z&se=2023-09-20T16%3A31%3A38Z&sp=r', 'system_logs/snapshot_capability/snapshot-capability.log': 'https://azureconstorage3630ce4c4.blob.core.windows.net/azureml/ExperimentRun/dcid.a952eb20-4134-49fe-9016-3eb604efb6ff/system_logs/snapshot_capability/snapshot-capability.log?sv=2019-07-07&sr=b&sig=nLhX3Y6BI7snyRhQ3azPExsb8pjyXTp8aIi2xJ4y%2FZ8%3D&skoid=93afd85a-f35b-452f-9e9c-45330f173f88&sktid=55587992-a704-412d-a52c-ce2a96ed3cf4&skt=2023-09-20T07%3A45%3A27Z&ske=2023-09-21T15%3A55%3A27Z&sks=b&skv=2019-07-07&st=2023-09-20T08%3A21%3A38Z&se=2023-09-20T16%3A31%3A38Z&sp=r'}, 'submittedBy': 'Khanh Van Le'}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## Wait for pipeline completion - we can see the pipeline status using this code\n",
        "if not schedule_pipeline_flag:\n",
        "    pipeline_run.wait_for_completion(show_output=True, raise_on_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [5.5] Scheduling Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Publish Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if schedule_pipeline_flag:\n",
        "    published_pipeline_run = pipeline.publish(\n",
        "        name=pipeline_name, \n",
        "        description=pipeline_name)\n",
        "    pub_pipeline_id = published_pipeline_run.id\n",
        "    print(\"Newly published pipeline id: {}\".format(published_pipeline_run.id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Schedule Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If any file content is changed or any new file is added to folder then it will trigger the pipeline.\n",
        "# Currently we are checking the content change in updateStatus.json file.\n",
        "# Copy new data to folder 'InputData/InputRawFiles/transaction_data/' and then update time and status in updateStatus.json file,\n",
        "# pipeline will be triggered\n",
        "if schedule_pipeline_flag:\n",
        "    schedule_info = Schedule.load_yaml(\n",
        "        workspace=ws,\n",
        "        filename=scheduler_config_file\n",
        "    )\n",
        "    schedule_name = f'{pipeline_name}_schedule'\n",
        "    schedule = Schedule.create(\n",
        "        workspace=ws, \n",
        "        name=schedule_name,\n",
        "        pipeline_id=pub_pipeline_id, \n",
        "        experiment_name=experiment_name+'_scheduler',\n",
        "        datastore=default_store,\n",
        "        wait_for_provisioning=True,\n",
        "        description=schedule_info.get(\"description\"),\n",
        "        path_on_datastore=data_path_to_monitor,\n",
        "        pipeline_parameters=schedule_info.get(\"pipeline_parameters\")\n",
        "    )\n",
        "    schedule_id = schedule.id\n",
        "    print(\"Created schedule {} with id: {}\".format(schedule_name, schedule.id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get all schedules for a given pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pub_pipeline_id = '378b2153-9af9-43e9-8dc6-5a3197432891'\n",
        "# schedules = Schedule.list(ws, pipeline_id=pub_pipeline_id)\n",
        "\n",
        "# print(\"Found these schedules for the pipeline id {}:\".format(pub_pipeline_id))\n",
        "# for schedule in schedules: \n",
        "#     print(schedule.id)\n",
        "\n",
        "# # print(\"Schedule id to be used for schedule operations: {}\".format(schedule_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get all schedules in your workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Use active_only=False to get all schedules including disabled schedules\n",
        "# schedules = Schedule.list(ws, active_only=True) \n",
        "# print(\"Your workspace has the following schedules set up:\")\n",
        "# for schedule in schedules:\n",
        "#     print(\"{} (Published pipeline: {}\".format(schedule.id, schedule.pipeline_id))\n",
        "# schedule_id = schedule.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Disable the schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def disable_by_schedule_id(ws, schedule_id):\n",
        "#     s = next(s for s in Schedule.list(ws) if s.id == schedule_id)\n",
        "#     s.disable()\n",
        "#     return s\n",
        "\n",
        "# def disable_by_schedule_name(ws, schedule_name):\n",
        "#     s = next(s for s in Schedule.list(ws) if s.name == schedule_name)\n",
        "#     s.disable()\n",
        "#     return s\n",
        "\n",
        "# disable_by_schedule_id(ws, 'f98435ff-c140-4d43-b918-90139c48ed95')\n",
        "# disable_by_schedule_id(ws, schedule_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enable the Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def enable_by_schedule_id(ws, schedule_id):\n",
        "#     s = next(s for s in Schedule.list(ws) if s.id == schedule_id)\n",
        "#     s.enable()\n",
        "#     return s\n",
        "\n",
        "# def enable_by_schedule_name(ws, schedule_name):\n",
        "#     s = next(s for s in Schedule.list(ws) if s.name == schedule_name)\n",
        "#     s.enable()\n",
        "#     return s\n",
        "\n",
        "# enable_by_schedule_id(ws, 'f98435ff-c140-4d43-b918-90139c48ed95')\n",
        "# enable_by_schedule_id(ws, schedule_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [5.6] Interpret Pipeline Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For automated ML runs, to access the charts from a previous run, use the experiment name:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [5.5.1] Get Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from azureml.pipeline.core import PipelineRun\n",
        "# from azureml.train.automl.run import AutoMLRun\n",
        "\n",
        "# experiment = Experiment(ws, experiment_name)\n",
        "# # pipeline_run = pipeline_run.id\n",
        "# pipeline_run = PipelineRun(experiment, 'a5e588a0-4879-4b60-8ad8-24ef5cd15c81')\n",
        "# automl_step_run = pipeline_run.find_step_run('Model_Training_Testing')[0]\n",
        "# automl_run = AutoMLRun(experiment, automl_step_run.id)\n",
        "# best_run, model = automl_run.get_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from azureml.interpret import ExplanationClient\n",
        "\n",
        "# client = ExplanationClient.from_run(best_run)\n",
        "# engineered_explanations = client.download_model_explanation(raw=False)\n",
        "# dict_feat_imp = engineered_explanations.get_feature_importance_dict()\n",
        "# df_feat_imp = pd.DataFrame(dict_feat_imp.items(), columns=['Feature Name', 'Importance Value'])\n",
        "# df_feat_imp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# client = ExplanationClient.from_run(best_run)\n",
        "# raw_explanations = client.download_model_explanation(raw=True)\n",
        "# dict_feat_imp = raw_explanations.get_feature_importance_dict()\n",
        "# df_feat_imp = pd.DataFrame(dict_feat_imp.items(), columns=['Feature Name', 'Importance Value'])\n",
        "# df_feat_imp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get best model based on metric\n",
        "# automl_run.get_best_child(metrics='f1-score-macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from azureml.widgets import RunDetails\n",
        "# from azureml.core.run import Run\n",
        "\n",
        "# experiment = Experiment (workspace, experiment_name)\n",
        "# run_id = pipeline_run.id #replace with run_ID\n",
        "# run = Run(experiment, run_id)\n",
        "# RunDetails(run).show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
