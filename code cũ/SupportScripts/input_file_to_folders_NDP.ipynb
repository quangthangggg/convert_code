{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset, Datastore\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.runconfig import DockerConfiguration\n",
        "from azureml.pipeline.core import PipelineData, PipelineParameter\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.core import Experiment\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from pathlib import Path"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# from joblib import Parallel, delayed\n",
        "# import multiprocessing as mp\n",
        "\n",
        "# from tqdm.notebook import tqdm\n",
        "# tqdm.pandas()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Input data raw files (path inside the blob container)\n",
        "input_full_raw_data_path = 'InputData/FullRawFiles'\n",
        "\n",
        "# output path\n",
        "# Note: This notebook helps put new data into Validation and Monitoring folder structure.\n",
        "# Due to data inconsistency, there are codes help convert data type, select columns,... into original dataset\n",
        "# For example: in Demographics, there are different between CIF_CREATE data type\n",
        "# output_raw_data_path = 'InputData/InputRawFiles'\n",
        "output_raw_data_path = 'InputData/InputRawFiles_VM'"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "subscription_id = 'bdea9b80-e147-4dd1-9e22-afd1228b6d1a'\n",
        "resource_group = 'casa-churn-analysis-ey-demo'\n",
        "workspace_name = 'AzureConnection'\n",
        "\n",
        "# Create the Workspace\n",
        "ws = Workspace(subscription_id, resource_group, workspace_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 202 ms, sys: 13 ms, total: 215 ms\nWall time: 405 ms\n"
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "user_name = 'qa'"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "blob_storage_name = 'azureconstorage3630ce4c4'\n",
        "blob_container = 'azureml'\n",
        "blob_key = 'OaxkL/pE4/XVJ1nJiZABE8wvsfSN434A66MiyiyywQOhYEDRYXS0t9DP+xLDT+RS8Alcygkszd+W+ASt54cY2w=='\n",
        "\n",
        "# Datastore name: We can see dataAssets created in this, this will also refer to the default blob storage specified\n",
        "datastore_name = f\"{user_name}_blob_datastore\""
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Blob storage to Datastore\n",
        "blob_data_store = Datastore.register_azure_blob_container(\n",
        "    workspace = ws, \n",
        "    datastore_name = datastore_name,\n",
        "    account_name = blob_storage_name,\n",
        "    container_name = blob_container,\n",
        "    account_key = blob_key\n",
        ")\n",
        "# Default Datastore\n",
        "# default_store = ws.get_default_datastore()\n",
        "default_store = blob_data_store"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 91.7 ms, sys: 0 ns, total: 91.7 ms\nWall time: 755 ms\n"
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataAssetName_suffix = ''"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# List of input csv files to read - use only csv files\n",
        "map_dataAsset_inputFile = {\n",
        "    # 'transaction_data' : 'TRANSACTION_VM.csv',\n",
        "    # 'demographics_data' : 'Customer_Demographic.csv',\n",
        "    'demographics_data_v' : 'CUSTOMER_DEMOGRAPHICS_V.csv',\n",
        "    # 'demographics_data_m' : 'CUSTOMER_DEMOGRAPHICS_M.csv', # encoding = 'latin1'\n",
        "    # 'account_data' : 'AR_LCS_VM.csv'\n",
        "}\n",
        "\n",
        "# Create and Register Data Assets\n",
        "for dataAssetName, inputFileName in map_dataAsset_inputFile.items():\n",
        "    dataAssetNameOnAzure = f'{user_name}_{dataAssetName}{dataAssetName_suffix}'\n",
        "    csv_path = [(blob_data_store, f\"{input_full_raw_data_path}/{inputFileName}\")]\n",
        "    globals()[dataAssetName] = Dataset.Tabular.from_delimited_files(path=csv_path,encoding = 'latin1')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 287 ms, sys: 41.3 ms, total: 328 ms\nWall time: 1.19 s\n"
        }
      ],
      "execution_count": 115,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", 50)\n",
        "pd.options.display.float_format = '{:.3f}'.format"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "def fun_datetime_format(date_str):\n",
        "    ''' This function convert any type of datetime format into date object of \"%d-%b-%y\".\n",
        "     Ex: Convert this 29-Oct-14 12:00:00 AM into 29-Oct-14 \n",
        "     Or 03-12-07 0:00 into 03-Dec-07.\n",
        "     \n",
        "     Need to enter the specific type of datetime. Reference: https://docs.python.org/3/library/datetime.html '''\n",
        "    \n",
        "    date_type = \"%d-%b-%y %H:%M:%S %p\"\n",
        "\n",
        "    datetime_obj = datetime.datetime.strptime(date_str, date_type)\n",
        "    formatted_date = datetime_obj.strftime(\"%d-%b-%y\")\n",
        "    return formatted_date"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def fun_convert_datetime_to_object(date_time):\n",
        "    ''' This function converts datetime type into object type.\n",
        "    Ex: 2007-11-08 into 08-Nov-07'''\n",
        "    date_obj = date_time.date().strftime(\"%d-%b-%y\")\n",
        "    return date_obj"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demographics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_demo_v = demographics_data_v.to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": 128,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_demo_m = demographics_data_m.to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": 163,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "customer_segment = {\n",
        "    'Phổ thông':'Standard',\n",
        "    'Ph? thông':'Standard',\n",
        "    'Premier' : 'Premier',\n",
        "    'Thân thiết':'Vip',\n",
        "    'Thân thi?t':'Vip', \n",
        "    'Elite': 'Elite', \n",
        "    'Private': 'Private',\n",
        "    'Chưa phân loại': 'Standard',\n",
        "    'Phá»• thÃ´ng': 'Standard',\n",
        "    'ThÃ¢n thiáº¿t':'Vip', \n",
        "    'ChÆ°a phÃ¢n loáº¡i': 'Standard'\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 131,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_demo_v['CUSTOMER_SEGMENT'] = df_demo_v['CUSTOMER_SEGMENT'].map(customer_segment)\n",
        "df_demo_v['CIF_CREATE'] = df_demo_v['CIF_CREATE'].apply(fun_datetime_format)\n",
        "df_demo_v.rename(columns={'ï»¿MOBILE_FLAG':'MOBILE_FLAG'},inplace=True)\n",
        "df_demo_v['MOBILE_FLAG'] = df_demo_v['MOBILE_FLAG'].astype({'MOBILE_FLAG': 'int64'}) "
      ],
      "outputs": [],
      "execution_count": 132,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_demo_m['CUSTOMER_SEGMENT'] = df_demo_m['CUSTOMER_SEGMENT'].map(customer_segment)\n",
        "df_demo_m['CIF_CREATE'] = df_demo_m['CIF_CREATE'].apply(fun_convert_datetime_to_object)"
      ],
      "outputs": [],
      "execution_count": 164,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# cus_lst_v = set(df_demo_v.HASHED_CIF.unique())\n",
        "# cus_lst_m = set(df_demo_m.HASHED_CIF.unique())\n",
        "# cus_lst_same = cus_lst_v.intersection(cus_lst_m)\n",
        "# tbl_m = df_demo_m[~df_demo_m['HASHED_CIF'].isin(cus_lst_same)]\n",
        "# tbl_v = df_demo_v[~df_demo_v['HASHED_CIF'].isin(cus_lst_same)]\n",
        "# tbl_m_same = df_demo_m[df_demo_m['HASHED_CIF'].isin(cus_lst_same)]\n",
        "# tbl_v_same = df_demo_v[df_demo_v['HASHED_CIF'].isin(cus_lst_same)]"
      ],
      "outputs": [],
      "execution_count": 167,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_demo = pd.concat([df_demo_v, df_demo_m], ignore_index=True).drop_duplicates(subset='HASHED_CIF').reset_index(drop=True)\n",
        "df_demo.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 189,
          "data": {
            "text/plain": "(394043, 19)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 189,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataAssetName = 'demographic_data'\n",
        "inputFileName = 'Customer_Demographic.csv'\n",
        "# Write to azure file system folder as parquet files\n",
        "local_path = f'../tempData/{inputFileName}'\n",
        "file_path = f'{local_path}/{inputFileName}'\n",
        "Path(local_path).mkdir(parents=True, exist_ok=True)\n",
        "df_demo.to_csv(file_path, index=False, header=True,encoding='utf-8')\n",
        "\n",
        "# Upload the files to blob data store\n",
        "target_path = f'{output_raw_data_path}/{dataAssetName}'\n",
        "blob_data_store.upload_files([file_path], target_path=target_path, overwrite=True, show_progress=True)\n",
        "    \n",
        "if os.path.exists(\"../tempData\"):\n",
        "    os.system(\"rm -rf \"+\"../tempData\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 1 files\nUploading ../tempData/Customer_Demographic.csv/Customer_Demographic.csv\nUploaded ../tempData/Customer_Demographic.csv/Customer_Demographic.csv, 1 files out of an estimated total of 1\nUploaded 1 files\n"
        }
      ],
      "execution_count": 190,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Account"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_acc = account_data.to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_acc['DATE_OF_STATUS'] = df_acc['DATE_OF_STATUS'].apply(fun_datetime_format)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataAssetName = 'account_data'\n",
        "inputFileName = 'AR_LCS.csv'\n",
        "# Write to azure file system folder as parquet files\n",
        "local_path = f'../tempData/{inputFileName}'\n",
        "file_path = f'{local_path}/{inputFileName}'\n",
        "Path(local_path).mkdir(parents=True, exist_ok=True)\n",
        "df_acc.to_csv(file_path, index=False, header=True)\n",
        "\n",
        "# Upload the files to blob data store\n",
        "target_path = f'{output_raw_data_path}/{dataAssetName}'\n",
        "blob_data_store.upload_files([file_path], target_path=target_path, overwrite=True, show_progress=True)\n",
        "    \n",
        "if os.path.exists(\"../tempData\"):\n",
        "    os.system(\"rm -rf \"+\"../tempData\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\"datastore.upload_files\" is deprecated after version 1.0.69. Please use \"FileDatasetFactory.upload_directory\" instead. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 1 files\nUploading ../tempData/AR_LCS.csv/AR_LCS.csv\nUploaded ../tempData/AR_LCS.csv/AR_LCS.csv, 1 files out of an estimated total of 1\nUploaded 1 files\n"
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transaction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_trx = transaction_data.to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# def applyParallel(dfGrouped, func, args_lst=[], njobs=2*mp.cpu_count()-1):\n",
        "#     ''' Run the grouped data in parallel. '''\n",
        "#     retLst = Parallel(n_jobs = njobs, verbose = 2)(delayed(func)(group, args_lst) for name, group in dfGrouped)\n",
        "#     return pd.concat(retLst)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def fun_write_yearly_monthly_file(df_year, arg_list):\n",
        "    table_name = arg_list[0]\n",
        "    month = df_year['THANG'].unique()[0]\n",
        "    year = df_year['NAM'].unique()[0]\n",
        "    if len(str(month)) == 1:\n",
        "        month = '0' + str(month)\n",
        "    \n",
        "    # Write to azure file system folder as parquet files\n",
        "    local_path = f'../tempData/{year}_{month}'\n",
        "    file_path = f'{local_path}/{year}_{month}.csv'\n",
        "    Path(local_path).mkdir(parents=True, exist_ok=True)\n",
        "    df_year.to_csv(file_path, index=False, header=True)\n",
        "    \n",
        "    # Upload the files to blob data store\n",
        "    target_path = f'{output_raw_data_path}/{table_name}/{year}_{month}'\n",
        "    blob_data_store.upload_files([file_path], target_path=target_path, overwrite=True, show_progress=True)\n",
        "\n",
        "    print(f'Uploaded {year}_{month}.csv to {target_path}.')\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'year' : [year], 'month' : [month], 'blob_path' : [target_path]}, \n",
        "        columns=['year', 'month', 'blob_path']\n",
        "    )\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Parallel function\n",
        "table_name = 'transaction_data'\n",
        "# df = applyParallel(\n",
        "#     dfGrouped=df_trx.groupby(['NAM', 'THANG']), \n",
        "#     func=fun_write_yearly_monthly_file, \n",
        "#     args_lst=[table_name],\n",
        "# )\n",
        "\n",
        "# Non-parallel function - only for testing\n",
        "df_status = df_trx.groupby(['NAM', 'THANG']).apply(fun_write_yearly_monthly_file, ([table_name]))\n",
        "df_status.reset_index(drop=True, inplace=True)\n",
        "if os.path.exists(\"../tempData\"):\n",
        "    os.system(\"rm -rf \"+\"../tempData\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\"datastore.upload_files\" is deprecated after version 1.0.69. Please use \"FileDatasetFactory.upload_directory\" instead. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 1 files\nUploading ../tempData/2022_06/2022_06.csv\nUploaded ../tempData/2022_06/2022_06.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2022_06.csv to InputData/InputRawFiles_VM/transaction_data/2022_06.\nUploading an estimated of 1 files\nUploading ../tempData/2022_07/2022_07.csv\nUploaded ../tempData/2022_07/2022_07.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2022_07.csv to InputData/InputRawFiles_VM/transaction_data/2022_07.\nUploading an estimated of 1 files\nUploading ../tempData/2022_08/2022_08.csv\nUploaded ../tempData/2022_08/2022_08.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2022_08.csv to InputData/InputRawFiles_VM/transaction_data/2022_08.\nUploading an estimated of 1 files\nUploading ../tempData/2022_09/2022_09.csv\nUploaded ../tempData/2022_09/2022_09.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2022_09.csv to InputData/InputRawFiles_VM/transaction_data/2022_09.\nUploading an estimated of 1 files\nUploading ../tempData/2022_10/2022_10.csv\nUploaded ../tempData/2022_10/2022_10.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2022_10.csv to InputData/InputRawFiles_VM/transaction_data/2022_10.\nUploading an estimated of 1 files\nUploading ../tempData/2022_11/2022_11.csv\nUploaded ../tempData/2022_11/2022_11.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2022_11.csv to InputData/InputRawFiles_VM/transaction_data/2022_11.\nUploading an estimated of 1 files\nUploading ../tempData/2022_12/2022_12.csv\nUploaded ../tempData/2022_12/2022_12.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2022_12.csv to InputData/InputRawFiles_VM/transaction_data/2022_12.\nUploading an estimated of 1 files\nUploading ../tempData/2023_01/2023_01.csv\nUploaded ../tempData/2023_01/2023_01.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2023_01.csv to InputData/InputRawFiles_VM/transaction_data/2023_01.\nUploading an estimated of 1 files\nUploading ../tempData/2023_02/2023_02.csv\nUploaded ../tempData/2023_02/2023_02.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2023_02.csv to InputData/InputRawFiles_VM/transaction_data/2023_02.\nUploading an estimated of 1 files\nUploading ../tempData/2023_03/2023_03.csv\nUploaded ../tempData/2023_03/2023_03.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2023_03.csv to InputData/InputRawFiles_VM/transaction_data/2023_03.\nUploading an estimated of 1 files\nUploading ../tempData/2023_04/2023_04.csv\nUploaded ../tempData/2023_04/2023_04.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2023_04.csv to InputData/InputRawFiles_VM/transaction_data/2023_04.\nUploading an estimated of 1 files\nUploading ../tempData/2023_05/2023_05.csv\nUploaded ../tempData/2023_05/2023_05.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2023_05.csv to InputData/InputRawFiles_VM/transaction_data/2023_05.\nUploading an estimated of 1 files\nUploading ../tempData/2023_06/2023_06.csv\nUploaded ../tempData/2023_06/2023_06.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nUploaded 2023_06.csv to InputData/InputRawFiles_VM/transaction_data/2023_06.\nCPU times: user 3min 14s, sys: 8.01 s, total: 3min 22s\nWall time: 5min 27s\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Bad pipe message: %s [b'\\x96(\\x81\\x9ddaD\\xf1\\x89}\\x1dyJ`\\x08\\x05\\xb0\\x14 \\x13\\xbf\\xc4\\x97\\x06\\xf2\\x99\\x8bWr\\xff\\xf4\\xcc \\x8c']\nBad pipe message: %s [b\"\\xaaC\\xc8\\xa9\\xd8\\xbb\\xe8\\xf2%\\x8ex\\xa7V\\x9b\\xf7\\xb7\\xd6\\x8e\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x00\", b'\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00']\nBad pipe message: %s [b\"\\xbf\\xa9\\xbf\\x02\\x13\\xa3b(\\xfa\\xf7\\r\\x06tQ\\x9a~\\xfe\\xef\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\", b'\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08']\nBad pipe message: %s [b'\\xa3\\x025\\x15\\xa3i\\xedr\\x0f\\xd2\\xe5\\n\\xf67\\x0e\\xe3j\\xe4\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05']\nBad pipe message: %s [b'\\x9d\\xbf\\xb0`9\\ns.\\xd94k\\xb6\\x8a~\\xc2\\xc1\\x98\\xa9\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0', b'\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16']\nBad pipe message: %s [b'1/\\x81V\\x86\\x8b\\xcf\\xd8\\x15\\xf9\\xf6\\x91\\xa87\\xc60\\xf9\\x12\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00']\nBad pipe message: %s [b'\\xf3\\x9en\\x8b=\\x8a\\x92\\xa1bK\\xe7\\xe2x\\xab\\x16\\n%\\xa5\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00']\nBad pipe message: %s [b'\\xda\\x8f\\xb4r`\\xf8*_\\xcc\\xd9qzb\\x17\\xab\\xa4\\\\\\xf3\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A']\nBad pipe message: %s [b'\\x08\\n\\x08\\x0b\\x08\\x04\\x08', b'\\x06\\x04\\x01\\x05']\nBad pipe message: %s [b'\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00']\nBad pipe message: %s [b'', b'\\x03\\x03']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s [b'\\x05\\x02\\x06']\nBad pipe message: %s [b'd\\xdd\\x16\\x83\\x9b\\x11\\x1an\\xc5\\xd4Px>\\xbd@Lxw\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85']\nBad pipe message: %s [b\"\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\", b'\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e']\n"
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_status.head(50)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}