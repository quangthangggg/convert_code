{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import time\n",
        "import argparse\n",
        "import os\n",
        "from azureml.core import Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook specific imports\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing as mp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fun_drop_duplicates(df, table_name):\n",
        "    '''Dropping duplicate rows in the data.'''\n",
        "    rows_before = df.shape[0]\n",
        "    df.drop_duplicates(inplace=True,ignore_index=True)\n",
        "    rows_after = df.shape[0]\n",
        "    print(f'Number of duplicate rows dropped: {rows_before-rows_after} from table {table_name}')\n",
        "    print(f'Percentage of duplicate rows dropped: {round(100*(rows_before-rows_after)/rows_before, 2)} % from table {table_name}')\n",
        "    print('-'*30)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fun_convert_vnt_to_eng(df_translation, df):\n",
        "\n",
        "    # convert csv to dict\n",
        "    vn_en_dict = dict(zip(df_translation.VN, df_translation.EN))\n",
        "\n",
        "    # translates columns if needed\n",
        "    df.rename(columns=vn_en_dict, inplace=True)\n",
        "    \n",
        "    # translates rows if needed\n",
        "    ## transaction table need no conversion\n",
        "    if 'AMOUNT_TRANSACTION' not in df.columns:\n",
        "        df.replace(vn_en_dict, inplace=True)\n",
        "        print('Conversion Complete')\n",
        "        \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fun_get_missing_perc(df):\n",
        "    '''Function provides information on the columns with missing data.'''\n",
        "    df_missing = df.isna().sum().reset_index()\n",
        "    df_missing = df_missing[df_missing[0] > 0]#.drop(columns=['level_0'])\n",
        "    df_missing['percentage_missing'] = round(100*df_missing[0]/df.shape[0], 2)\n",
        "    df_missing.columns = ['column_name', 'missing_value', 'percentage_missing']\n",
        "    print('Percentage of missing data in Columns: ')\n",
        "    print(df_missing)\n",
        "    return df_missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def applyParallel(dfGrouped, func, args_lst=[], njobs=mp.cpu_count()-1):\n",
        "    ''' Run the grouped data in parallel. '''\n",
        "    retLst = Parallel(n_jobs = njobs, verbose = 2)(delayed(func)(group, args_lst) for name, group in dfGrouped)\n",
        "    return pd.concat(retLst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def human_format(num):\n",
        "    num = float('{:.3g}'.format(num))\n",
        "    magnitude = 0\n",
        "    while abs(num) >= 1000:\n",
        "        magnitude += 1\n",
        "        num /= 1000.0\n",
        "    return '{}{}'.format('{:f}'.format(num).rstrip('0').rstrip('.'), ['', 'K', 'M', 'B', 'T'][magnitude])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fun_get_flag_distribution(tbl, column_name, output_file):\n",
        "    '''Get value counts of flag with percentage.'''\n",
        "    df1 = tbl[column_name].value_counts().rename_axis(column_name).reset_index(name='actualCount')\n",
        "    df2 = tbl[column_name].value_counts(normalize=True).mul(100).round(1)\\\n",
        "            .rename_axis(column_name).reset_index(name='percentage')\n",
        "    df3 = pd.merge(df1, df2, on=column_name)\n",
        "    df3['percentage'] = df3['percentage'].astype(str) + ' %'\n",
        "    df3['abbrCount'] = df3['actualCount'].apply(lambda x: human_format(x))\n",
        "    df3['file_name'] = output_file\n",
        "    df3 = df3[['file_name', column_name, 'abbrCount', 'percentage', 'actualCount']]\n",
        "    return df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fun_write_file(tbl, output_path, output_file, run, csv=False):\n",
        "    print('-'*30)\n",
        "    print(f'Number of rows and columns in the file {output_file} are: {tbl.shape[0]} & {tbl.shape[1]}')\n",
        " \n",
        "    # for logging as metric\n",
        "    tbl_col_names = ', '.join(tbl.columns)\n",
        "    columns = ['file_name', 'TotalColumns', 'TotalRowsAbbr', 'TotalRows', 'ColumnNames']\n",
        "    list_of_values = [output_file, tbl.shape[1], human_format(tbl.shape[0]), tbl.shape[0], tbl_col_names]\n",
        "    df_info = pd.DataFrame([list_of_values], columns=columns)\n",
        "\n",
        "    # run.log('TotalRowsAbbr', human_format(tbl.shape[0]))\n",
        "    # run.log('TotalRows', tbl.shape[0])\n",
        "    # run.log('TotalColumns', tbl.shape[1])\n",
        "    # cols = ', '.join(tbl.columns)\n",
        "    # run.log('Columns', cols)\n",
        "    print('Columns in data:', tbl.columns)\n",
        "    if 'HASHED_CIF' in tbl.columns:\n",
        "        print(f'Number of unique customer ids in the file are: {tbl[\"HASHED_CIF\"].nunique()}')\n",
        "        # run.log('NumofCustomersAbbr', human_format(tbl[\"HASHED_CIF\"].nunique()))\n",
        "        # run.log('NumofCustomers', tbl[\"HASHED_CIF\"].nunique())\n",
        "        df_info['NumofCustomersAbbr'] = human_format(tbl[\"HASHED_CIF\"].nunique())\n",
        "        df_info['NumofCustomers'] = tbl[\"HASHED_CIF\"].nunique()\n",
        "    run.log_table('Basic_Data_Information', df_info.to_dict('list'))\n",
        "\n",
        "    if 'churn_flag' in tbl.columns:\n",
        "        print(f'Target distribution in file: {tbl[\"churn_flag\"].value_counts()}')\n",
        "        df_flag = fun_get_flag_distribution(tbl, 'churn_flag', output_file)\n",
        "        run.log_table(\"Churn_Flag_Distribution\", df_flag.to_dict('list'))\n",
        "    if 'churn_flag_predicted' in tbl.columns:\n",
        "        print(f'Target distribution in file: {tbl[\"churn_flag_predicted\"].value_counts()}')\n",
        "        df_flag = fun_get_flag_distribution(tbl, 'churn_flag_predicted', output_file)\n",
        "        run.log_table(\"Predicted_Churn_Flag_Distribution\", df_flag.to_dict('list'))\n",
        "    if not (output_path is None):\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        print(\"File %s created\" % output_path)\n",
        "        if csv:\n",
        "            output_path = f'{output_path}/{output_file}.csv' #+ \"/processed.parquet\"\n",
        "            write_df = tbl.to_csv(output_path, index=False)\n",
        "        else: \n",
        "            output_path = f'{output_path}/{output_file}.parquet' #+ \"/processed.parquet\"\n",
        "            write_df = tbl.to_parquet(output_path)\n",
        "    else:\n",
        "        print(\"-\"*50)\n",
        "        print(\"File %s already created\" % output_path)\n",
        "        print(\"-\"*50)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
